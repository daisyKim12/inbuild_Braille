{"cells":[{"cell_type":"markdown","metadata":{"id":"B7tAP-7aNjFL"},"source":["# Side Walk Guide - Train Model\n","\n","In this notebook we will train a neural network to take an input image, and output x values corresponding to a target.\n","\n","We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4987,"status":"ok","timestamp":1700126230601,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"z7h3sSeaNjFR"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import glob\n","import PIL.Image\n","import os\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"G36EvUvhNjFT"},"source":["### Download and extract data\n","\n","Before you start, you should upload the ``training_dataset.zip`` file that you created in the ``data_collection.ipynb``\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1700126230602,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"OTUV72qSNjFU","outputId":"61ba8db3-6fe7-4258-e192-9841e0e69caa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  training_data1.zip\n","   creating: training_data1/\n","  inflating: __MACOSX/._training_data1  \n","  inflating: training_data1/74-481.jpg  \n","  inflating: __MACOSX/training_data1/._74-481.jpg  \n","  inflating: training_data1/11-370.jpg  \n","  inflating: __MACOSX/training_data1/._11-370.jpg  \n","  inflating: training_data1/112-34.jpg  \n","  inflating: __MACOSX/training_data1/._112-34.jpg  \n","  inflating: training_data1/116-413.jpg  \n","  inflating: __MACOSX/training_data1/._116-413.jpg  \n","  inflating: training_data1/127-65.jpg  \n","  inflating: __MACOSX/training_data1/._127-65.jpg  \n","  inflating: training_data1/76-340.jpg  \n","  inflating: __MACOSX/training_data1/._76-340.jpg  \n","  inflating: training_data1/55-213.jpg  \n","  inflating: __MACOSX/training_data1/._55-213.jpg  \n","  inflating: training_data1/124-155.jpg  \n","  inflating: __MACOSX/training_data1/._124-155.jpg  \n","  inflating: training_data1/36-42.jpg  \n","  inflating: __MACOSX/training_data1/._36-42.jpg  \n","  inflating: training_data1/126-640.jpg  \n","  inflating: __MACOSX/training_data1/._126-640.jpg  \n","  inflating: training_data1/57-47.jpg  \n","  inflating: __MACOSX/training_data1/._57-47.jpg  \n","  inflating: training_data1/155-255.jpg  \n","  inflating: __MACOSX/training_data1/._155-255.jpg  \n","  inflating: training_data1/107-428.jpg  \n","  inflating: __MACOSX/training_data1/._107-428.jpg  \n","  inflating: training_data1/73-207.jpg  \n","  inflating: __MACOSX/training_data1/._73-207.jpg  \n","  inflating: training_data1/161-391.jpg  \n","  inflating: __MACOSX/training_data1/._161-391.jpg  \n","  inflating: training_data1/42-278.jpg  \n","  inflating: __MACOSX/training_data1/._42-278.jpg  \n","  inflating: training_data1/82-236.jpg  \n","  inflating: __MACOSX/training_data1/._82-236.jpg  \n","  inflating: training_data1/106-216.jpg  \n","  inflating: __MACOSX/training_data1/._106-216.jpg  \n","  inflating: training_data1/41-581.jpg  \n","  inflating: __MACOSX/training_data1/._41-581.jpg  \n","  inflating: training_data1/138-410.jpg  \n","  inflating: __MACOSX/training_data1/._138-410.jpg  \n","  inflating: training_data1/103-63.jpg  \n","  inflating: __MACOSX/training_data1/._103-63.jpg  \n","  inflating: training_data1/81-14.jpg  \n","  inflating: __MACOSX/training_data1/._81-14.jpg  \n","  inflating: training_data1/1-25.jpg  \n","  inflating: __MACOSX/training_data1/._1-25.jpg  \n","  inflating: training_data1/101-246.jpg  \n","  inflating: __MACOSX/training_data1/._101-246.jpg  \n","  inflating: training_data1/96-549.jpg  \n","  inflating: __MACOSX/training_data1/._96-549.jpg  \n","  inflating: training_data1/59-569.jpg  \n","  inflating: __MACOSX/training_data1/._59-569.jpg  \n","  inflating: training_data1/31-318.jpg  \n","  inflating: __MACOSX/training_data1/._31-318.jpg  \n","  inflating: training_data1/92-582.jpg  \n","  inflating: __MACOSX/training_data1/._92-582.jpg  \n","  inflating: training_data1/15-169.jpg  \n","  inflating: __MACOSX/training_data1/._15-169.jpg  \n","  inflating: training_data1/39-108.jpg  \n","  inflating: __MACOSX/training_data1/._39-108.jpg  \n","  inflating: training_data1/52-490.jpg  \n","  inflating: __MACOSX/training_data1/._52-490.jpg  \n","  inflating: training_data1/110-295.jpg  \n","  inflating: __MACOSX/training_data1/._110-295.jpg  \n","  inflating: training_data1/6-223.jpg  \n","  inflating: __MACOSX/training_data1/._6-223.jpg  \n","  inflating: training_data1/135-632.jpg  \n","  inflating: __MACOSX/training_data1/._135-632.jpg  \n","  inflating: training_data1/29-452.jpg  \n","  inflating: __MACOSX/training_data1/._29-452.jpg  \n","  inflating: training_data1/19-206.jpg  \n","  inflating: __MACOSX/training_data1/._19-206.jpg  \n","  inflating: training_data1/50-555.jpg  \n","  inflating: __MACOSX/training_data1/._50-555.jpg  \n","  inflating: training_data1/62-629.jpg  \n","  inflating: __MACOSX/training_data1/._62-629.jpg  \n","  inflating: training_data1/104-329.jpg  \n","  inflating: __MACOSX/training_data1/._104-329.jpg  \n","  inflating: training_data1/26-575.jpg  \n","  inflating: __MACOSX/training_data1/._26-575.jpg  \n","  inflating: training_data1/88-535.jpg  \n","  inflating: __MACOSX/training_data1/._88-535.jpg  \n","  inflating: training_data1/63-23.jpg  \n","  inflating: __MACOSX/training_data1/._63-23.jpg  \n","  inflating: training_data1/25-412.jpg  \n","  inflating: __MACOSX/training_data1/._25-412.jpg  \n","  inflating: training_data1/5-384.jpg  \n","  inflating: __MACOSX/training_data1/._5-384.jpg  \n","  inflating: training_data1/130-37.jpg  \n","  inflating: __MACOSX/training_data1/._130-37.jpg  \n","  inflating: training_data1/30-57.jpg  \n","  inflating: __MACOSX/training_data1/._30-57.jpg  \n","  inflating: training_data1/46-173.jpg  \n","  inflating: __MACOSX/training_data1/._46-173.jpg  \n","  inflating: training_data1/66-13.jpg  \n","  inflating: __MACOSX/training_data1/._66-13.jpg  \n","  inflating: training_data1/61-388.jpg  \n","  inflating: __MACOSX/training_data1/._61-388.jpg  \n","  inflating: training_data1/118-23.jpg  \n","  inflating: __MACOSX/training_data1/._118-23.jpg  \n","  inflating: training_data1/163-236.jpg  \n","  inflating: __MACOSX/training_data1/._163-236.jpg  \n","  inflating: training_data1/1-57.jpg  \n","  inflating: __MACOSX/training_data1/._1-57.jpg  \n","  inflating: training_data1/17-584.jpg  \n","  inflating: __MACOSX/training_data1/._17-584.jpg  \n","  inflating: training_data1/108-633.jpg  \n","  inflating: __MACOSX/training_data1/._108-633.jpg  \n","  inflating: training_data1/100-54.jpg  \n","  inflating: __MACOSX/training_data1/._100-54.jpg  \n","  inflating: training_data1/147-455.jpg  \n","  inflating: __MACOSX/training_data1/._147-455.jpg  \n","  inflating: training_data1/35-571.jpg  \n","  inflating: __MACOSX/training_data1/._35-571.jpg  \n","  inflating: training_data1/20-420.jpg  \n","  inflating: __MACOSX/training_data1/._20-420.jpg  \n","  inflating: training_data1/2-107.jpg  \n","  inflating: __MACOSX/training_data1/._2-107.jpg  \n","  inflating: training_data1/145-92.jpg  \n","  inflating: __MACOSX/training_data1/._145-92.jpg  \n","  inflating: training_data1/53-636.jpg  \n","  inflating: __MACOSX/training_data1/._53-636.jpg  \n","  inflating: training_data1/5-382.jpg  \n","  inflating: __MACOSX/training_data1/._5-382.jpg  \n","  inflating: training_data1/78-398.jpg  \n","  inflating: __MACOSX/training_data1/._78-398.jpg  \n","  inflating: training_data1/40-332.jpg  \n","  inflating: __MACOSX/training_data1/._40-332.jpg  \n","  inflating: training_data1/84-46.jpg  \n","  inflating: __MACOSX/training_data1/._84-46.jpg  \n","  inflating: training_data1/90-33.jpg  \n","  inflating: __MACOSX/training_data1/._90-33.jpg  \n","  inflating: training_data1/95-600.jpg  \n","  inflating: __MACOSX/training_data1/._95-600.jpg  \n","  inflating: training_data1/32-521.jpg  \n","  inflating: __MACOSX/training_data1/._32-521.jpg  \n","  inflating: training_data1/4-386.jpg  \n","  inflating: __MACOSX/training_data1/._4-386.jpg  \n","  inflating: training_data1/58-309.jpg  \n","  inflating: __MACOSX/training_data1/._58-309.jpg  \n","  inflating: training_data1/114-605.jpg  \n","  inflating: __MACOSX/training_data1/._114-605.jpg  \n","  inflating: training_data1/49-332.jpg  \n","  inflating: __MACOSX/training_data1/._49-332.jpg  \n","  inflating: training_data1/115-158.jpg  \n","  inflating: __MACOSX/training_data1/._115-158.jpg  \n","  inflating: training_data1/80-633.jpg  \n","  inflating: __MACOSX/training_data1/._80-633.jpg  \n","  inflating: training_data1/156-522.jpg  \n","  inflating: __MACOSX/training_data1/._156-522.jpg  \n","  inflating: training_data1/83-451.jpg  \n","  inflating: __MACOSX/training_data1/._83-451.jpg  \n","  inflating: training_data1/154-13.jpg  \n","  inflating: __MACOSX/training_data1/._154-13.jpg  \n","  inflating: training_data1/16-424.jpg  \n","  inflating: __MACOSX/training_data1/._16-424.jpg  \n","  inflating: training_data1/89-631.jpg  \n","  inflating: __MACOSX/training_data1/._89-631.jpg  \n","  inflating: training_data1/129-528.jpg  \n","  inflating: __MACOSX/training_data1/._129-528.jpg  \n","  inflating: training_data1/93-49.jpg  \n","  inflating: __MACOSX/training_data1/._93-49.jpg  \n","  inflating: training_data1/60-89.jpg  \n","  inflating: __MACOSX/training_data1/._60-89.jpg  \n","  inflating: training_data1/65-408.jpg  \n","  inflating: __MACOSX/training_data1/._65-408.jpg  \n","  inflating: training_data1/45-23.jpg  \n","  inflating: __MACOSX/training_data1/._45-23.jpg  \n","  inflating: training_data1/119-180.jpg  \n","  inflating: __MACOSX/training_data1/._119-180.jpg  \n","  inflating: training_data1/51-282.jpg  \n","  inflating: __MACOSX/training_data1/._51-282.jpg  \n","  inflating: training_data1/23-489.jpg  \n","  inflating: __MACOSX/training_data1/._23-489.jpg  \n","  inflating: training_data1/125-477.jpg  \n","  inflating: __MACOSX/training_data1/._125-477.jpg  \n","  inflating: training_data1/111-570.jpg  \n","  inflating: __MACOSX/training_data1/._111-570.jpg  \n","  inflating: training_data1/14-459.jpg  \n","  inflating: __MACOSX/training_data1/._14-459.jpg  \n","  inflating: training_data1/48-46.jpg  \n","  inflating: __MACOSX/training_data1/._48-46.jpg  \n","  inflating: training_data1/109-46.jpg  \n","  inflating: __MACOSX/training_data1/._109-46.jpg  \n","  inflating: training_data1/131-325.jpg  \n","  inflating: __MACOSX/training_data1/._131-325.jpg  \n","  inflating: training_data1/162-549.jpg  \n","  inflating: __MACOSX/training_data1/._162-549.jpg  \n","  inflating: training_data1/67-312.jpg  \n","  inflating: __MACOSX/training_data1/._67-312.jpg  \n","  inflating: training_data1/128-252.jpg  \n","  inflating: __MACOSX/training_data1/._128-252.jpg  \n","  inflating: training_data1/134-467.jpg  \n","  inflating: __MACOSX/training_data1/._134-467.jpg  \n","  inflating: training_data1/6-273.jpg  \n","  inflating: __MACOSX/training_data1/._6-273.jpg  \n","  inflating: training_data1/146-263.jpg  \n","  inflating: __MACOSX/training_data1/._146-263.jpg  \n","  inflating: training_data1/136-158.jpg  \n","  inflating: __MACOSX/training_data1/._136-158.jpg  \n","  inflating: training_data1/75-65.jpg  \n","  inflating: __MACOSX/training_data1/._75-65.jpg  \n","  inflating: training_data1/85-341.jpg  \n","  inflating: __MACOSX/training_data1/._85-341.jpg  \n","  inflating: training_data1/47-381.jpg  \n","  inflating: __MACOSX/training_data1/._47-381.jpg  \n","  inflating: training_data1/98-405.jpg  \n","  inflating: __MACOSX/training_data1/._98-405.jpg  \n","  inflating: training_data1/18-75.jpg  \n","  inflating: __MACOSX/training_data1/._18-75.jpg  \n","  inflating: training_data1/71-625.jpg  \n","  inflating: __MACOSX/training_data1/._71-625.jpg  \n","  inflating: training_data1/34-420.jpg  \n","  inflating: __MACOSX/training_data1/._34-420.jpg  \n","  inflating: training_data1/121-247.jpg  \n","  inflating: __MACOSX/training_data1/._121-247.jpg  \n","  inflating: training_data1/64-166.jpg  \n","  inflating: __MACOSX/training_data1/._64-166.jpg  \n","  inflating: training_data1/105-618.jpg  \n","  inflating: __MACOSX/training_data1/._105-618.jpg  \n","  inflating: training_data1/28-189.jpg  \n","  inflating: __MACOSX/training_data1/._28-189.jpg  \n","  inflating: training_data1/113-339.jpg  \n","  inflating: __MACOSX/training_data1/._113-339.jpg  \n","  inflating: training_data1/149-344.jpg  \n","  inflating: __MACOSX/training_data1/._149-344.jpg  \n","  inflating: training_data1/102-489.jpg  \n","  inflating: __MACOSX/training_data1/._102-489.jpg  \n","  inflating: training_data1/133-193.jpg  \n","  inflating: __MACOSX/training_data1/._133-193.jpg  \n","  inflating: training_data1/22-273.jpg  \n","  inflating: __MACOSX/training_data1/._22-273.jpg  \n","  inflating: training_data1/157-74.jpg  \n","  inflating: __MACOSX/training_data1/._157-74.jpg  \n","  inflating: training_data1/123-596.jpg  \n","  inflating: __MACOSX/training_data1/._123-596.jpg  \n","  inflating: training_data1/56-425.jpg  \n","  inflating: __MACOSX/training_data1/._56-425.jpg  \n","  inflating: training_data1/144-612.jpg  \n","  inflating: __MACOSX/training_data1/._144-612.jpg  \n","  inflating: training_data1/3-132.jpg  \n","  inflating: __MACOSX/training_data1/._3-132.jpg  \n","  inflating: training_data1/143-450.jpg  \n","  inflating: __MACOSX/training_data1/._143-450.jpg  \n","  inflating: training_data1/99-611.jpg  \n","  inflating: __MACOSX/training_data1/._99-611.jpg  \n","  inflating: training_data1/3-74.jpg  \n","  inflating: __MACOSX/training_data1/._3-74.jpg  \n","  inflating: training_data1/68-556.jpg  \n","  inflating: __MACOSX/training_data1/._68-556.jpg  \n","  inflating: training_data1/141-556.jpg  \n","  inflating: __MACOSX/training_data1/._141-556.jpg  \n","  inflating: training_data1/24-181.jpg  \n","  inflating: __MACOSX/training_data1/._24-181.jpg  \n","  inflating: training_data1/70-523.jpg  \n","  inflating: __MACOSX/training_data1/._70-523.jpg  \n","  inflating: training_data1/137-288.jpg  \n","  inflating: __MACOSX/training_data1/._137-288.jpg  \n","  inflating: training_data1/150-534.jpg  \n","  inflating: __MACOSX/training_data1/._150-534.jpg  \n","  inflating: training_data1/44-626.jpg  \n","  inflating: __MACOSX/training_data1/._44-626.jpg  \n","  inflating: training_data1/158-334.jpg  \n","  inflating: __MACOSX/training_data1/._158-334.jpg  \n","  inflating: training_data1/151-121.jpg  \n","  inflating: __MACOSX/training_data1/._151-121.jpg  \n","  inflating: training_data1/8-521.jpg  \n","  inflating: __MACOSX/training_data1/._8-521.jpg  \n","  inflating: training_data1/27-18.jpg  \n","  inflating: __MACOSX/training_data1/._27-18.jpg  \n","  inflating: training_data1/86-591.jpg  \n","  inflating: __MACOSX/training_data1/._86-591.jpg  \n","  inflating: training_data1/9-73.jpg  \n","  inflating: __MACOSX/training_data1/._9-73.jpg  \n","  inflating: training_data1/139-93.jpg  \n","  inflating: __MACOSX/training_data1/._139-93.jpg  \n","  inflating: training_data1/10-180.jpg  \n","  inflating: __MACOSX/training_data1/._10-180.jpg  \n","  inflating: training_data1/79-457.jpg  \n","  inflating: __MACOSX/training_data1/._79-457.jpg  \n","  inflating: training_data1/37-218.jpg  \n","  inflating: __MACOSX/training_data1/._37-218.jpg  \n","  inflating: training_data1/120-426.jpg  \n","  inflating: __MACOSX/training_data1/._120-426.jpg  \n","  inflating: training_data1/12-77.jpg  \n","  inflating: __MACOSX/training_data1/._12-77.jpg  \n","  inflating: training_data1/13-295.jpg  \n","  inflating: __MACOSX/training_data1/._13-295.jpg  \n","  inflating: training_data1/132-536.jpg  \n","  inflating: __MACOSX/training_data1/._132-536.jpg  \n","  inflating: training_data1/87-150.jpg  \n","  inflating: __MACOSX/training_data1/._87-150.jpg  \n","  inflating: training_data1/77-549.jpg  \n","  inflating: __MACOSX/training_data1/._77-549.jpg  \n","  inflating: training_data1/153-608.jpg  \n","  inflating: __MACOSX/training_data1/._153-608.jpg  \n","  inflating: training_data1/152-423.jpg  \n","  inflating: __MACOSX/training_data1/._152-423.jpg  \n","  inflating: training_data1/2-96.jpg  \n","  inflating: __MACOSX/training_data1/._2-96.jpg  \n","  inflating: training_data1/160-56.jpg  \n","  inflating: __MACOSX/training_data1/._160-56.jpg  \n","  inflating: training_data1/21-81.jpg  \n","  inflating: __MACOSX/training_data1/._21-81.jpg  \n","  inflating: training_data1/94-322.jpg  \n","  inflating: __MACOSX/training_data1/._94-322.jpg  \n","  inflating: training_data1/91-298.jpg  \n","  inflating: __MACOSX/training_data1/._91-298.jpg  \n","  inflating: training_data1/140-320.jpg  \n","  inflating: __MACOSX/training_data1/._140-320.jpg  \n","  inflating: training_data1/117-627.jpg  \n","  inflating: __MACOSX/training_data1/._117-627.jpg  \n","  inflating: training_data1/54-27.jpg  \n","  inflating: __MACOSX/training_data1/._54-27.jpg  \n","  inflating: training_data1/122-321.jpg  \n","  inflating: __MACOSX/training_data1/._122-321.jpg  \n","  inflating: training_data1/148-81.jpg  \n","  inflating: __MACOSX/training_data1/._148-81.jpg  \n","  inflating: training_data1/38-489.jpg  \n","  inflating: __MACOSX/training_data1/._38-489.jpg  \n","  inflating: training_data1/4-259.jpg  \n","  inflating: __MACOSX/training_data1/._4-259.jpg  \n","  inflating: training_data1/7-371.jpg  \n","  inflating: __MACOSX/training_data1/._7-371.jpg  \n","  inflating: training_data1/72-21.jpg  \n","  inflating: __MACOSX/training_data1/._72-21.jpg  \n","  inflating: training_data1/33-202.jpg  \n","  inflating: __MACOSX/training_data1/._33-202.jpg  \n","  inflating: training_data1/97-85.jpg  \n","  inflating: __MACOSX/training_data1/._97-85.jpg  \n","  inflating: training_data1/69-280.jpg  \n","  inflating: __MACOSX/training_data1/._69-280.jpg  \n","  inflating: training_data1/159-533.jpg  \n","  inflating: __MACOSX/training_data1/._159-533.jpg  \n","  inflating: training_data1/43-514.jpg  \n","  inflating: __MACOSX/training_data1/._43-514.jpg  \n","  inflating: training_data1/142-192.jpg  \n","  inflating: __MACOSX/training_data1/._142-192.jpg  \n"]}],"source":["!unzip training_data1.zip\n","!mv training_data1 training_dataset\n","# !unzip training_dataset2 training_dataset"]},{"cell_type":"markdown","metadata":{"id":"KJBNVsLdNjFV"},"source":["### Create Dataset Instance\n","\n","Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n","is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n","we can use all of the torch data utilities"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1700126231224,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"S1CHn9g9NjFW","outputId":"dd1cf897-e93b-4c01-bc8a-30496c326a3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample 1 - Image Shape: torch.Size([3, 224, 224]), x: 4.233333110809326\n","Sample 2 - Image Shape: torch.Size([3, 224, 224]), x: 3.683333396911621\n","Sample 3 - Image Shape: torch.Size([3, 224, 224]), x: 3.4000000953674316\n","Sample 4 - Image Shape: torch.Size([3, 224, 224]), x: -3.0166666507720947\n","Sample 5 - Image Shape: torch.Size([3, 224, 224]), x: -4.2166666984558105\n"]}],"source":["def get_x(file_name):\n","    \"\"\"Gets the x value from the image filename\"\"\"\n","    token = file_name.split(\"-\")\n","    #print(token[0], token[1])\n","    return (float(int(token[1].split(\".\")[0]) - 300.0) * 5 / 300.0)\n","\n","class XLableAugmentImageTrainData(torch.utils.data.Dataset):\n","\n","    def __init__(self, directory, random_hflips=False):\n","        self.directory = directory\n","        self.random_hflips = random_hflips\n","        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n","        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","\n","        image = PIL.Image.open(image_path)\n","        x = float(get_x(os.path.basename(image_path)))\n","\n","        if float(np.random.rand(1)) > 0.5:\n","            image = transforms.functional.hflip(image)\n","            x = -x\n","\n","        image = self.color_jitter(image)\n","        image = transforms.functional.resize(image, (224, 224))\n","        image = transforms.functional.to_tensor(image)\n","        image = image.numpy()[::-1].copy()\n","        image = torch.from_numpy(image)\n","        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","        return image, torch.tensor([x]).float()\n","\n","dataset = XLableAugmentImageTrainData('training_dataset', random_hflips=False)\n","\n","for i in range(5):\n","    image, x = dataset[i]\n","    print(f\"Sample {i+1} - Image Shape: {image.shape}, x: {x.item()}\")"]},{"cell_type":"markdown","metadata":{"id":"BVKfkDLkNjFY"},"source":["### Split dataset into train and test sets\n","Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 80%-20%. The test set will be used to verify the accuracy of the model we train."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700126231224,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"jlzdcun6NjFZ"},"outputs":[],"source":["test_percent = 0.2\n","num_test = int(test_percent * len(dataset))\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"]},{"cell_type":"markdown","metadata":{"id":"wiqblvTDNjFa"},"source":["### Create data loaders to load data in batches\n","\n","We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700126231224,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"tp7EW9KQNjFb"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    shuffle=True,\n","    num_workers=2\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=16,\n","    shuffle=True,\n","    num_workers=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"4CAblercNjFc"},"source":["### Define Neural Network Model\n","\n","We use ResNet-18 model available on PyTorch TorchVision.\n","\n","In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n","\n","\n","More details on ResNet-18 : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\n","More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1700126231736,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"T2HzY-JuNjFc","outputId":"8802c127-87f5-436a-8c32-5fa626a062a4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 158MB/s]\n"]}],"source":["model = models.resnet18(pretrained=True)"]},{"cell_type":"markdown","metadata":{"id":"GTu0v3l3NjFd"},"source":["ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 1\n","\n","Finally, we transfer our model for execution on the GPU"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700126231737,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"TYnQRd5uNjFd"},"outputs":[],"source":["model.fc = torch.nn.Linear(512, 1)\n","device = torch.device('cuda')\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"ap7E4lriNjFe"},"source":["### Train Regression:\n","\n","We train for 50 epochs and save best model if the loss is reduced."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403568,"status":"ok","timestamp":1700126635303,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"},"user_tz":-540},"id":"GOxdxANCNjFe","outputId":"422bfc34-eedd-4b20-b737-005786cc745f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/70] - Batch [1/9] - Train Loss: 10.2647\n","Epoch [1/70] - Batch [2/9] - Train Loss: 11.4299\n","Epoch [1/70] - Batch [3/9] - Train Loss: 6.1027\n","Epoch [1/70] - Batch [4/9] - Train Loss: 25.8712\n","Epoch [1/70] - Batch [5/9] - Train Loss: 7.5970\n","Epoch [1/70] - Batch [6/9] - Train Loss: 7.7579\n","Epoch [1/70] - Batch [7/9] - Train Loss: 4.0129\n","Epoch [1/70] - Batch [8/9] - Train Loss: 8.4834\n","Epoch [1/70] - Batch [9/9] - Train Loss: 11.4146\n","Epoch [1/70] - Batch [1/3] - Test Loss: 14.4658\n","Epoch [1/70] - Batch [2/3] - Test Loss: 24.1736\n","Epoch [1/70] - Batch [3/3] - Test Loss: 24.6029\n","Epoch [1/70] - Train Loss: 10.3260, Test Loss: 21.0808\n","---<checkpoint saved>---21.08076349894206\n","Epoch [2/70] - Batch [1/9] - Train Loss: 4.1158\n","Epoch [2/70] - Batch [2/9] - Train Loss: 8.2349\n","Epoch [2/70] - Batch [3/9] - Train Loss: 6.9811\n","Epoch [2/70] - Batch [4/9] - Train Loss: 2.5713\n","Epoch [2/70] - Batch [5/9] - Train Loss: 2.9400\n","Epoch [2/70] - Batch [6/9] - Train Loss: 4.1763\n","Epoch [2/70] - Batch [7/9] - Train Loss: 1.8373\n","Epoch [2/70] - Batch [8/9] - Train Loss: 5.0571\n","Epoch [2/70] - Batch [9/9] - Train Loss: 4.0574\n","Epoch [2/70] - Batch [1/3] - Test Loss: 24.5987\n","Epoch [2/70] - Batch [2/3] - Test Loss: 13.6949\n","Epoch [2/70] - Batch [3/3] - Test Loss: 97.2403\n","Epoch [2/70] - Train Loss: 4.4412, Test Loss: 45.1780\n","Epoch [3/70] - Batch [1/9] - Train Loss: 2.6971\n","Epoch [3/70] - Batch [2/9] - Train Loss: 5.8103\n","Epoch [3/70] - Batch [3/9] - Train Loss: 2.5111\n","Epoch [3/70] - Batch [4/9] - Train Loss: 2.4943\n","Epoch [3/70] - Batch [5/9] - Train Loss: 8.9890\n","Epoch [3/70] - Batch [6/9] - Train Loss: 2.2756\n","Epoch [3/70] - Batch [7/9] - Train Loss: 3.6618\n","Epoch [3/70] - Batch [8/9] - Train Loss: 3.0483\n","Epoch [3/70] - Batch [9/9] - Train Loss: 3.2505\n","Epoch [3/70] - Batch [1/3] - Test Loss: 4.9839\n","Epoch [3/70] - Batch [2/3] - Test Loss: 5.5961\n","Epoch [3/70] - Batch [3/3] - Test Loss: 0.2375\n","Epoch [3/70] - Train Loss: 3.8598, Test Loss: 3.6058\n","---<checkpoint saved>---3.6058253894249597\n","Epoch [4/70] - Batch [1/9] - Train Loss: 3.5577\n","Epoch [4/70] - Batch [2/9] - Train Loss: 4.6247\n","Epoch [4/70] - Batch [3/9] - Train Loss: 1.7998\n","Epoch [4/70] - Batch [4/9] - Train Loss: 2.0440\n","Epoch [4/70] - Batch [5/9] - Train Loss: 2.5962\n","Epoch [4/70] - Batch [6/9] - Train Loss: 2.3307\n","Epoch [4/70] - Batch [7/9] - Train Loss: 2.6145\n","Epoch [4/70] - Batch [8/9] - Train Loss: 2.4647\n","Epoch [4/70] - Batch [9/9] - Train Loss: 1.9735\n","Epoch [4/70] - Batch [1/3] - Test Loss: 4.8782\n","Epoch [4/70] - Batch [2/3] - Test Loss: 6.4854\n","Epoch [4/70] - Batch [3/3] - Test Loss: 12.3473\n","Epoch [4/70] - Train Loss: 2.6673, Test Loss: 7.9036\n","Epoch [5/70] - Batch [1/9] - Train Loss: 2.9122\n","Epoch [5/70] - Batch [2/9] - Train Loss: 1.2061\n","Epoch [5/70] - Batch [3/9] - Train Loss: 2.1633\n","Epoch [5/70] - Batch [4/9] - Train Loss: 1.9389\n","Epoch [5/70] - Batch [5/9] - Train Loss: 9.2724\n","Epoch [5/70] - Batch [6/9] - Train Loss: 2.1940\n","Epoch [5/70] - Batch [7/9] - Train Loss: 0.5718\n","Epoch [5/70] - Batch [8/9] - Train Loss: 2.0658\n","Epoch [5/70] - Batch [9/9] - Train Loss: 2.1524\n","Epoch [5/70] - Batch [1/3] - Test Loss: 4.7570\n","Epoch [5/70] - Batch [2/3] - Test Loss: 2.6212\n","Epoch [5/70] - Batch [3/3] - Test Loss: 2.9325\n","Epoch [5/70] - Train Loss: 2.7196, Test Loss: 3.4369\n","---<checkpoint saved>---3.436891794204712\n","Epoch [6/70] - Batch [1/9] - Train Loss: 2.1953\n","Epoch [6/70] - Batch [2/9] - Train Loss: 2.0862\n","Epoch [6/70] - Batch [3/9] - Train Loss: 3.2027\n","Epoch [6/70] - Batch [4/9] - Train Loss: 1.1618\n","Epoch [6/70] - Batch [5/9] - Train Loss: 0.5592\n","Epoch [6/70] - Batch [6/9] - Train Loss: 2.8627\n","Epoch [6/70] - Batch [7/9] - Train Loss: 1.5163\n","Epoch [6/70] - Batch [8/9] - Train Loss: 1.6270\n","Epoch [6/70] - Batch [9/9] - Train Loss: 1.2966\n","Epoch [6/70] - Batch [1/3] - Test Loss: 1.5155\n","Epoch [6/70] - Batch [2/3] - Test Loss: 2.9539\n","Epoch [6/70] - Batch [3/3] - Test Loss: 4.7860\n","Epoch [6/70] - Train Loss: 1.8342, Test Loss: 3.0851\n","---<checkpoint saved>---3.085108002026876\n","Epoch [7/70] - Batch [1/9] - Train Loss: 1.1063\n","Epoch [7/70] - Batch [2/9] - Train Loss: 0.7423\n","Epoch [7/70] - Batch [3/9] - Train Loss: 0.8228\n","Epoch [7/70] - Batch [4/9] - Train Loss: 0.7276\n","Epoch [7/70] - Batch [5/9] - Train Loss: 1.3106\n","Epoch [7/70] - Batch [6/9] - Train Loss: 0.2923\n","Epoch [7/70] - Batch [7/9] - Train Loss: 1.6941\n","Epoch [7/70] - Batch [8/9] - Train Loss: 2.6877\n","Epoch [7/70] - Batch [9/9] - Train Loss: 0.2094\n","Epoch [7/70] - Batch [1/3] - Test Loss: 0.9742\n","Epoch [7/70] - Batch [2/3] - Test Loss: 1.0499\n","Epoch [7/70] - Batch [3/3] - Test Loss: 0.5505\n","Epoch [7/70] - Train Loss: 1.0659, Test Loss: 0.8582\n","---<checkpoint saved>---0.8582189679145813\n","Epoch [8/70] - Batch [1/9] - Train Loss: 0.3601\n","Epoch [8/70] - Batch [2/9] - Train Loss: 0.4285\n","Epoch [8/70] - Batch [3/9] - Train Loss: 0.5100\n","Epoch [8/70] - Batch [4/9] - Train Loss: 0.8039\n","Epoch [8/70] - Batch [5/9] - Train Loss: 2.1804\n","Epoch [8/70] - Batch [6/9] - Train Loss: 1.1525\n","Epoch [8/70] - Batch [7/9] - Train Loss: 1.6049\n","Epoch [8/70] - Batch [8/9] - Train Loss: 1.0458\n","Epoch [8/70] - Batch [9/9] - Train Loss: 0.7042\n","Epoch [8/70] - Batch [1/3] - Test Loss: 0.5448\n","Epoch [8/70] - Batch [2/3] - Test Loss: 1.7328\n","Epoch [8/70] - Batch [3/3] - Test Loss: 0.7978\n","Epoch [8/70] - Train Loss: 0.9767, Test Loss: 1.0251\n","Epoch [9/70] - Batch [1/9] - Train Loss: 0.4890\n","Epoch [9/70] - Batch [2/9] - Train Loss: 0.4535\n","Epoch [9/70] - Batch [3/9] - Train Loss: 1.7256\n","Epoch [9/70] - Batch [4/9] - Train Loss: 4.8695\n","Epoch [9/70] - Batch [5/9] - Train Loss: 1.2745\n","Epoch [9/70] - Batch [6/9] - Train Loss: 0.9811\n","Epoch [9/70] - Batch [7/9] - Train Loss: 0.6999\n","Epoch [9/70] - Batch [8/9] - Train Loss: 2.1972\n","Epoch [9/70] - Batch [9/9] - Train Loss: 0.4319\n","Epoch [9/70] - Batch [1/3] - Test Loss: 0.5449\n","Epoch [9/70] - Batch [2/3] - Test Loss: 2.0786\n","Epoch [9/70] - Batch [3/3] - Test Loss: 0.3116\n","Epoch [9/70] - Train Loss: 1.4580, Test Loss: 0.9784\n","Epoch [10/70] - Batch [1/9] - Train Loss: 0.0826\n","Epoch [10/70] - Batch [2/9] - Train Loss: 0.9288\n","Epoch [10/70] - Batch [3/9] - Train Loss: 0.7296\n","Epoch [10/70] - Batch [4/9] - Train Loss: 0.4330\n","Epoch [10/70] - Batch [5/9] - Train Loss: 2.6473\n","Epoch [10/70] - Batch [6/9] - Train Loss: 0.2376\n","Epoch [10/70] - Batch [7/9] - Train Loss: 1.2615\n","Epoch [10/70] - Batch [8/9] - Train Loss: 0.8346\n","Epoch [10/70] - Batch [9/9] - Train Loss: 0.6970\n","Epoch [10/70] - Batch [1/3] - Test Loss: 0.9623\n","Epoch [10/70] - Batch [2/3] - Test Loss: 0.8419\n","Epoch [10/70] - Batch [3/3] - Test Loss: 1.3468\n","Epoch [10/70] - Train Loss: 0.8724, Test Loss: 1.0503\n","Epoch [11/70] - Batch [1/9] - Train Loss: 0.0753\n","Epoch [11/70] - Batch [2/9] - Train Loss: 0.2142\n","Epoch [11/70] - Batch [3/9] - Train Loss: 0.2428\n","Epoch [11/70] - Batch [4/9] - Train Loss: 2.5566\n","Epoch [11/70] - Batch [5/9] - Train Loss: 0.1782\n","Epoch [11/70] - Batch [6/9] - Train Loss: 0.3028\n","Epoch [11/70] - Batch [7/9] - Train Loss: 0.5716\n","Epoch [11/70] - Batch [8/9] - Train Loss: 0.9545\n","Epoch [11/70] - Batch [9/9] - Train Loss: 5.1389\n","Epoch [11/70] - Batch [1/3] - Test Loss: 0.7908\n","Epoch [11/70] - Batch [2/3] - Test Loss: 0.7520\n","Epoch [11/70] - Batch [3/3] - Test Loss: 0.0150\n","Epoch [11/70] - Train Loss: 1.1372, Test Loss: 0.5193\n","---<checkpoint saved>---0.5192689818019668\n","Epoch [12/70] - Batch [1/9] - Train Loss: 1.9764\n","Epoch [12/70] - Batch [2/9] - Train Loss: 0.5555\n","Epoch [12/70] - Batch [3/9] - Train Loss: 0.5241\n","Epoch [12/70] - Batch [4/9] - Train Loss: 0.2793\n","Epoch [12/70] - Batch [5/9] - Train Loss: 0.7088\n","Epoch [12/70] - Batch [6/9] - Train Loss: 2.3572\n","Epoch [12/70] - Batch [7/9] - Train Loss: 0.8241\n","Epoch [12/70] - Batch [8/9] - Train Loss: 1.3470\n","Epoch [12/70] - Batch [9/9] - Train Loss: 3.0145\n","Epoch [12/70] - Batch [1/3] - Test Loss: 0.5709\n","Epoch [12/70] - Batch [2/3] - Test Loss: 0.7960\n","Epoch [12/70] - Batch [3/3] - Test Loss: 3.2849\n","Epoch [12/70] - Train Loss: 1.2874, Test Loss: 1.5506\n","Epoch [13/70] - Batch [1/9] - Train Loss: 0.2824\n","Epoch [13/70] - Batch [2/9] - Train Loss: 1.1096\n","Epoch [13/70] - Batch [3/9] - Train Loss: 0.4718\n","Epoch [13/70] - Batch [4/9] - Train Loss: 1.6708\n","Epoch [13/70] - Batch [5/9] - Train Loss: 0.9912\n","Epoch [13/70] - Batch [6/9] - Train Loss: 0.1817\n","Epoch [13/70] - Batch [7/9] - Train Loss: 5.9453\n","Epoch [13/70] - Batch [8/9] - Train Loss: 1.1397\n","Epoch [13/70] - Batch [9/9] - Train Loss: 2.2910\n","Epoch [13/70] - Batch [1/3] - Test Loss: 0.7519\n","Epoch [13/70] - Batch [2/3] - Test Loss: 0.5466\n","Epoch [13/70] - Batch [3/3] - Test Loss: 0.0235\n","Epoch [13/70] - Train Loss: 1.5648, Test Loss: 0.4407\n","---<checkpoint saved>---0.4406581502407789\n","Epoch [14/70] - Batch [1/9] - Train Loss: 0.5324\n","Epoch [14/70] - Batch [2/9] - Train Loss: 0.1748\n","Epoch [14/70] - Batch [3/9] - Train Loss: 0.4543\n","Epoch [14/70] - Batch [4/9] - Train Loss: 0.2924\n","Epoch [14/70] - Batch [5/9] - Train Loss: 0.3078\n","Epoch [14/70] - Batch [6/9] - Train Loss: 0.1733\n","Epoch [14/70] - Batch [7/9] - Train Loss: 0.7470\n","Epoch [14/70] - Batch [8/9] - Train Loss: 0.3850\n","Epoch [14/70] - Batch [9/9] - Train Loss: 0.4179\n","Epoch [14/70] - Batch [1/3] - Test Loss: 1.1209\n","Epoch [14/70] - Batch [2/3] - Test Loss: 0.4206\n","Epoch [14/70] - Batch [3/3] - Test Loss: 2.1503\n","Epoch [14/70] - Train Loss: 0.3872, Test Loss: 1.2306\n","Epoch [15/70] - Batch [1/9] - Train Loss: 2.2415\n","Epoch [15/70] - Batch [2/9] - Train Loss: 0.8215\n","Epoch [15/70] - Batch [3/9] - Train Loss: 0.5107\n","Epoch [15/70] - Batch [4/9] - Train Loss: 0.6656\n","Epoch [15/70] - Batch [5/9] - Train Loss: 0.1787\n","Epoch [15/70] - Batch [6/9] - Train Loss: 0.4931\n","Epoch [15/70] - Batch [7/9] - Train Loss: 0.3428\n","Epoch [15/70] - Batch [8/9] - Train Loss: 0.3239\n","Epoch [15/70] - Batch [9/9] - Train Loss: 0.6868\n","Epoch [15/70] - Batch [1/3] - Test Loss: 0.6456\n","Epoch [15/70] - Batch [2/3] - Test Loss: 0.7253\n","Epoch [15/70] - Batch [3/3] - Test Loss: 0.0011\n","Epoch [15/70] - Train Loss: 0.6961, Test Loss: 0.4573\n","Epoch [16/70] - Batch [1/9] - Train Loss: 0.9713\n","Epoch [16/70] - Batch [2/9] - Train Loss: 0.4127\n","Epoch [16/70] - Batch [3/9] - Train Loss: 0.4052\n","Epoch [16/70] - Batch [4/9] - Train Loss: 0.7222\n","Epoch [16/70] - Batch [5/9] - Train Loss: 0.8231\n","Epoch [16/70] - Batch [6/9] - Train Loss: 0.3002\n","Epoch [16/70] - Batch [7/9] - Train Loss: 0.2135\n","Epoch [16/70] - Batch [8/9] - Train Loss: 0.1524\n","Epoch [16/70] - Batch [9/9] - Train Loss: 0.3343\n","Epoch [16/70] - Batch [1/3] - Test Loss: 1.0304\n","Epoch [16/70] - Batch [2/3] - Test Loss: 0.3696\n","Epoch [16/70] - Batch [3/3] - Test Loss: 0.2977\n","Epoch [16/70] - Train Loss: 0.4816, Test Loss: 0.5659\n","Epoch [17/70] - Batch [1/9] - Train Loss: 0.9273\n","Epoch [17/70] - Batch [2/9] - Train Loss: 0.1472\n","Epoch [17/70] - Batch [3/9] - Train Loss: 0.1080\n","Epoch [17/70] - Batch [4/9] - Train Loss: 0.0674\n","Epoch [17/70] - Batch [5/9] - Train Loss: 0.6529\n","Epoch [17/70] - Batch [6/9] - Train Loss: 0.2425\n","Epoch [17/70] - Batch [7/9] - Train Loss: 0.2735\n","Epoch [17/70] - Batch [8/9] - Train Loss: 1.4768\n","Epoch [17/70] - Batch [9/9] - Train Loss: 0.2786\n","Epoch [17/70] - Batch [1/3] - Test Loss: 0.8766\n","Epoch [17/70] - Batch [2/3] - Test Loss: 0.9456\n","Epoch [17/70] - Batch [3/3] - Test Loss: 0.0085\n","Epoch [17/70] - Train Loss: 0.4638, Test Loss: 0.6102\n","Epoch [18/70] - Batch [1/9] - Train Loss: 1.5541\n","Epoch [18/70] - Batch [2/9] - Train Loss: 0.6181\n","Epoch [18/70] - Batch [3/9] - Train Loss: 0.1995\n","Epoch [18/70] - Batch [4/9] - Train Loss: 0.3184\n","Epoch [18/70] - Batch [5/9] - Train Loss: 0.1655\n","Epoch [18/70] - Batch [6/9] - Train Loss: 0.2985\n","Epoch [18/70] - Batch [7/9] - Train Loss: 0.1320\n","Epoch [18/70] - Batch [8/9] - Train Loss: 0.4900\n","Epoch [18/70] - Batch [9/9] - Train Loss: 2.5360\n","Epoch [18/70] - Batch [1/3] - Test Loss: 1.0185\n","Epoch [18/70] - Batch [2/3] - Test Loss: 0.4623\n","Epoch [18/70] - Batch [3/3] - Test Loss: 0.0896\n","Epoch [18/70] - Train Loss: 0.7013, Test Loss: 0.5235\n","Epoch [19/70] - Batch [1/9] - Train Loss: 0.7944\n","Epoch [19/70] - Batch [2/9] - Train Loss: 0.1729\n","Epoch [19/70] - Batch [3/9] - Train Loss: 0.0834\n","Epoch [19/70] - Batch [4/9] - Train Loss: 1.1538\n","Epoch [19/70] - Batch [5/9] - Train Loss: 0.3615\n","Epoch [19/70] - Batch [6/9] - Train Loss: 1.2558\n","Epoch [19/70] - Batch [7/9] - Train Loss: 0.1229\n","Epoch [19/70] - Batch [8/9] - Train Loss: 0.3454\n","Epoch [19/70] - Batch [9/9] - Train Loss: 0.0793\n","Epoch [19/70] - Batch [1/3] - Test Loss: 0.8345\n","Epoch [19/70] - Batch [2/3] - Test Loss: 1.3427\n","Epoch [19/70] - Batch [3/3] - Test Loss: 0.1327\n","Epoch [19/70] - Train Loss: 0.4855, Test Loss: 0.7700\n","Epoch [20/70] - Batch [1/9] - Train Loss: 0.5473\n","Epoch [20/70] - Batch [2/9] - Train Loss: 0.4255\n","Epoch [20/70] - Batch [3/9] - Train Loss: 2.1644\n","Epoch [20/70] - Batch [4/9] - Train Loss: 0.2654\n","Epoch [20/70] - Batch [5/9] - Train Loss: 0.4424\n","Epoch [20/70] - Batch [6/9] - Train Loss: 0.2170\n","Epoch [20/70] - Batch [7/9] - Train Loss: 0.6035\n","Epoch [20/70] - Batch [8/9] - Train Loss: 1.1149\n","Epoch [20/70] - Batch [9/9] - Train Loss: 1.1847\n","Epoch [20/70] - Batch [1/3] - Test Loss: 0.6381\n","Epoch [20/70] - Batch [2/3] - Test Loss: 0.9063\n","Epoch [20/70] - Batch [3/3] - Test Loss: 1.1592\n","Epoch [20/70] - Train Loss: 0.7739, Test Loss: 0.9012\n","Epoch [21/70] - Batch [1/9] - Train Loss: 0.2112\n","Epoch [21/70] - Batch [2/9] - Train Loss: 0.1863\n","Epoch [21/70] - Batch [3/9] - Train Loss: 0.5058\n","Epoch [21/70] - Batch [4/9] - Train Loss: 0.5304\n","Epoch [21/70] - Batch [5/9] - Train Loss: 0.4799\n","Epoch [21/70] - Batch [6/9] - Train Loss: 1.5023\n","Epoch [21/70] - Batch [7/9] - Train Loss: 0.4283\n","Epoch [21/70] - Batch [8/9] - Train Loss: 0.1786\n","Epoch [21/70] - Batch [9/9] - Train Loss: 0.9656\n","Epoch [21/70] - Batch [1/3] - Test Loss: 0.8272\n","Epoch [21/70] - Batch [2/3] - Test Loss: 0.6061\n","Epoch [21/70] - Batch [3/3] - Test Loss: 0.0021\n","Epoch [21/70] - Train Loss: 0.5543, Test Loss: 0.4785\n","Epoch [22/70] - Batch [1/9] - Train Loss: 0.2043\n","Epoch [22/70] - Batch [2/9] - Train Loss: 0.4542\n","Epoch [22/70] - Batch [3/9] - Train Loss: 0.5013\n","Epoch [22/70] - Batch [4/9] - Train Loss: 0.0867\n","Epoch [22/70] - Batch [5/9] - Train Loss: 0.1094\n","Epoch [22/70] - Batch [6/9] - Train Loss: 0.5073\n","Epoch [22/70] - Batch [7/9] - Train Loss: 0.1074\n","Epoch [22/70] - Batch [8/9] - Train Loss: 0.6882\n","Epoch [22/70] - Batch [9/9] - Train Loss: 0.0535\n","Epoch [22/70] - Batch [1/3] - Test Loss: 1.0056\n","Epoch [22/70] - Batch [2/3] - Test Loss: 0.7066\n","Epoch [22/70] - Batch [3/3] - Test Loss: 0.3221\n","Epoch [22/70] - Train Loss: 0.3014, Test Loss: 0.6781\n","Epoch [23/70] - Batch [1/9] - Train Loss: 0.0814\n","Epoch [23/70] - Batch [2/9] - Train Loss: 0.2673\n","Epoch [23/70] - Batch [3/9] - Train Loss: 0.1413\n","Epoch [23/70] - Batch [4/9] - Train Loss: 0.4464\n","Epoch [23/70] - Batch [5/9] - Train Loss: 0.1011\n","Epoch [23/70] - Batch [6/9] - Train Loss: 0.3352\n","Epoch [23/70] - Batch [7/9] - Train Loss: 1.0217\n","Epoch [23/70] - Batch [8/9] - Train Loss: 0.6907\n","Epoch [23/70] - Batch [9/9] - Train Loss: 1.3779\n","Epoch [23/70] - Batch [1/3] - Test Loss: 0.7201\n","Epoch [23/70] - Batch [2/3] - Test Loss: 0.1911\n","Epoch [23/70] - Batch [3/3] - Test Loss: 3.7346\n","Epoch [23/70] - Train Loss: 0.4959, Test Loss: 1.5486\n","Epoch [24/70] - Batch [1/9] - Train Loss: 0.4618\n","Epoch [24/70] - Batch [2/9] - Train Loss: 0.1665\n","Epoch [24/70] - Batch [3/9] - Train Loss: 0.9939\n","Epoch [24/70] - Batch [4/9] - Train Loss: 0.2860\n","Epoch [24/70] - Batch [5/9] - Train Loss: 0.4817\n","Epoch [24/70] - Batch [6/9] - Train Loss: 1.7786\n","Epoch [24/70] - Batch [7/9] - Train Loss: 0.4355\n","Epoch [24/70] - Batch [8/9] - Train Loss: 0.4323\n","Epoch [24/70] - Batch [9/9] - Train Loss: 0.1110\n","Epoch [24/70] - Batch [1/3] - Test Loss: 0.2326\n","Epoch [24/70] - Batch [2/3] - Test Loss: 0.7817\n","Epoch [24/70] - Batch [3/3] - Test Loss: 0.1128\n","Epoch [24/70] - Train Loss: 0.5719, Test Loss: 0.3757\n","---<checkpoint saved>---0.3757198800643285\n","Epoch [25/70] - Batch [1/9] - Train Loss: 0.1104\n","Epoch [25/70] - Batch [2/9] - Train Loss: 0.0631\n","Epoch [25/70] - Batch [3/9] - Train Loss: 0.2649\n","Epoch [25/70] - Batch [4/9] - Train Loss: 0.1423\n","Epoch [25/70] - Batch [5/9] - Train Loss: 2.5261\n","Epoch [25/70] - Batch [6/9] - Train Loss: 0.3923\n","Epoch [25/70] - Batch [7/9] - Train Loss: 0.8514\n","Epoch [25/70] - Batch [8/9] - Train Loss: 0.2420\n","Epoch [25/70] - Batch [9/9] - Train Loss: 0.4237\n","Epoch [25/70] - Batch [1/3] - Test Loss: 0.6119\n","Epoch [25/70] - Batch [2/3] - Test Loss: 0.4146\n","Epoch [25/70] - Batch [3/3] - Test Loss: 0.3040\n","Epoch [25/70] - Train Loss: 0.5573, Test Loss: 0.4435\n","Epoch [26/70] - Batch [1/9] - Train Loss: 0.6427\n","Epoch [26/70] - Batch [2/9] - Train Loss: 0.0983\n","Epoch [26/70] - Batch [3/9] - Train Loss: 0.5469\n","Epoch [26/70] - Batch [4/9] - Train Loss: 0.6395\n","Epoch [26/70] - Batch [5/9] - Train Loss: 0.2429\n","Epoch [26/70] - Batch [6/9] - Train Loss: 0.2924\n","Epoch [26/70] - Batch [7/9] - Train Loss: 1.1896\n","Epoch [26/70] - Batch [8/9] - Train Loss: 0.1401\n","Epoch [26/70] - Batch [9/9] - Train Loss: 0.3974\n","Epoch [26/70] - Batch [1/3] - Test Loss: 0.2362\n","Epoch [26/70] - Batch [2/3] - Test Loss: 0.8197\n","Epoch [26/70] - Batch [3/3] - Test Loss: 0.2731\n","Epoch [26/70] - Train Loss: 0.4655, Test Loss: 0.4430\n","Epoch [27/70] - Batch [1/9] - Train Loss: 0.3801\n","Epoch [27/70] - Batch [2/9] - Train Loss: 1.9719\n","Epoch [27/70] - Batch [3/9] - Train Loss: 0.2841\n","Epoch [27/70] - Batch [4/9] - Train Loss: 0.6655\n","Epoch [27/70] - Batch [5/9] - Train Loss: 0.4500\n","Epoch [27/70] - Batch [6/9] - Train Loss: 2.2290\n","Epoch [27/70] - Batch [7/9] - Train Loss: 1.3385\n","Epoch [27/70] - Batch [8/9] - Train Loss: 0.6284\n","Epoch [27/70] - Batch [9/9] - Train Loss: 0.1795\n","Epoch [27/70] - Batch [1/3] - Test Loss: 0.9465\n","Epoch [27/70] - Batch [2/3] - Test Loss: 1.5391\n","Epoch [27/70] - Batch [3/3] - Test Loss: 0.0612\n","Epoch [27/70] - Train Loss: 0.9030, Test Loss: 0.8490\n","Epoch [28/70] - Batch [1/9] - Train Loss: 1.5812\n","Epoch [28/70] - Batch [2/9] - Train Loss: 0.5390\n","Epoch [28/70] - Batch [3/9] - Train Loss: 0.3629\n","Epoch [28/70] - Batch [4/9] - Train Loss: 1.4218\n","Epoch [28/70] - Batch [5/9] - Train Loss: 3.0205\n","Epoch [28/70] - Batch [6/9] - Train Loss: 1.6056\n","Epoch [28/70] - Batch [7/9] - Train Loss: 0.6436\n","Epoch [28/70] - Batch [8/9] - Train Loss: 1.4013\n","Epoch [28/70] - Batch [9/9] - Train Loss: 0.3615\n","Epoch [28/70] - Batch [1/3] - Test Loss: 0.7379\n","Epoch [28/70] - Batch [2/3] - Test Loss: 1.7185\n","Epoch [28/70] - Batch [3/3] - Test Loss: 0.0315\n","Epoch [28/70] - Train Loss: 1.2153, Test Loss: 0.8293\n","Epoch [29/70] - Batch [1/9] - Train Loss: 0.3753\n","Epoch [29/70] - Batch [2/9] - Train Loss: 1.4890\n","Epoch [29/70] - Batch [3/9] - Train Loss: 0.4938\n","Epoch [29/70] - Batch [4/9] - Train Loss: 1.2104\n","Epoch [29/70] - Batch [5/9] - Train Loss: 0.6266\n","Epoch [29/70] - Batch [6/9] - Train Loss: 0.3934\n","Epoch [29/70] - Batch [7/9] - Train Loss: 0.4193\n","Epoch [29/70] - Batch [8/9] - Train Loss: 0.6143\n","Epoch [29/70] - Batch [9/9] - Train Loss: 0.4152\n","Epoch [29/70] - Batch [1/3] - Test Loss: 1.0936\n","Epoch [29/70] - Batch [2/3] - Test Loss: 0.3205\n","Epoch [29/70] - Batch [3/3] - Test Loss: 0.0121\n","Epoch [29/70] - Train Loss: 0.6708, Test Loss: 0.4754\n","Epoch [30/70] - Batch [1/9] - Train Loss: 0.4262\n","Epoch [30/70] - Batch [2/9] - Train Loss: 1.5053\n","Epoch [30/70] - Batch [3/9] - Train Loss: 0.1228\n","Epoch [30/70] - Batch [4/9] - Train Loss: 0.2107\n","Epoch [30/70] - Batch [5/9] - Train Loss: 0.4565\n","Epoch [30/70] - Batch [6/9] - Train Loss: 0.5187\n","Epoch [30/70] - Batch [7/9] - Train Loss: 0.2872\n","Epoch [30/70] - Batch [8/9] - Train Loss: 0.8726\n","Epoch [30/70] - Batch [9/9] - Train Loss: 0.4158\n","Epoch [30/70] - Batch [1/3] - Test Loss: 0.4960\n","Epoch [30/70] - Batch [2/3] - Test Loss: 1.1168\n","Epoch [30/70] - Batch [3/3] - Test Loss: 0.4308\n","Epoch [30/70] - Train Loss: 0.5351, Test Loss: 0.6812\n","Epoch [31/70] - Batch [1/9] - Train Loss: 0.6140\n","Epoch [31/70] - Batch [2/9] - Train Loss: 0.1559\n","Epoch [31/70] - Batch [3/9] - Train Loss: 0.6046\n","Epoch [31/70] - Batch [4/9] - Train Loss: 0.9066\n","Epoch [31/70] - Batch [5/9] - Train Loss: 1.1294\n","Epoch [31/70] - Batch [6/9] - Train Loss: 0.3647\n","Epoch [31/70] - Batch [7/9] - Train Loss: 0.1412\n","Epoch [31/70] - Batch [8/9] - Train Loss: 0.0266\n","Epoch [31/70] - Batch [9/9] - Train Loss: 1.4385\n","Epoch [31/70] - Batch [1/3] - Test Loss: 1.5416\n","Epoch [31/70] - Batch [2/3] - Test Loss: 0.7220\n","Epoch [31/70] - Batch [3/3] - Test Loss: 0.3031\n","Epoch [31/70] - Train Loss: 0.5979, Test Loss: 0.8555\n","Epoch [32/70] - Batch [1/9] - Train Loss: 0.1638\n","Epoch [32/70] - Batch [2/9] - Train Loss: 0.2981\n","Epoch [32/70] - Batch [3/9] - Train Loss: 0.0812\n","Epoch [32/70] - Batch [4/9] - Train Loss: 1.0764\n","Epoch [32/70] - Batch [5/9] - Train Loss: 0.1252\n","Epoch [32/70] - Batch [6/9] - Train Loss: 0.6937\n","Epoch [32/70] - Batch [7/9] - Train Loss: 0.1180\n","Epoch [32/70] - Batch [8/9] - Train Loss: 1.0255\n","Epoch [32/70] - Batch [9/9] - Train Loss: 0.1234\n","Epoch [32/70] - Batch [1/3] - Test Loss: 0.4153\n","Epoch [32/70] - Batch [2/3] - Test Loss: 0.6502\n","Epoch [32/70] - Batch [3/3] - Test Loss: 0.0347\n","Epoch [32/70] - Train Loss: 0.4117, Test Loss: 0.3667\n","---<checkpoint saved>---0.36671971529722214\n","Epoch [33/70] - Batch [1/9] - Train Loss: 0.0966\n","Epoch [33/70] - Batch [2/9] - Train Loss: 0.1058\n","Epoch [33/70] - Batch [3/9] - Train Loss: 0.0603\n","Epoch [33/70] - Batch [4/9] - Train Loss: 0.1187\n","Epoch [33/70] - Batch [5/9] - Train Loss: 0.1723\n","Epoch [33/70] - Batch [6/9] - Train Loss: 0.1143\n","Epoch [33/70] - Batch [7/9] - Train Loss: 0.2201\n","Epoch [33/70] - Batch [8/9] - Train Loss: 0.1049\n","Epoch [33/70] - Batch [9/9] - Train Loss: 0.0755\n","Epoch [33/70] - Batch [1/3] - Test Loss: 0.6201\n","Epoch [33/70] - Batch [2/3] - Test Loss: 0.2551\n","Epoch [33/70] - Batch [3/3] - Test Loss: 0.1910\n","Epoch [33/70] - Train Loss: 0.1187, Test Loss: 0.3554\n","---<checkpoint saved>---0.3553757518529892\n","Epoch [34/70] - Batch [1/9] - Train Loss: 0.1590\n","Epoch [34/70] - Batch [2/9] - Train Loss: 0.0681\n","Epoch [34/70] - Batch [3/9] - Train Loss: 0.3422\n","Epoch [34/70] - Batch [4/9] - Train Loss: 0.7670\n","Epoch [34/70] - Batch [5/9] - Train Loss: 0.2043\n","Epoch [34/70] - Batch [6/9] - Train Loss: 0.3208\n","Epoch [34/70] - Batch [7/9] - Train Loss: 0.3875\n","Epoch [34/70] - Batch [8/9] - Train Loss: 0.2055\n","Epoch [34/70] - Batch [9/9] - Train Loss: 0.9202\n","Epoch [34/70] - Batch [1/3] - Test Loss: 0.1823\n","Epoch [34/70] - Batch [2/3] - Test Loss: 0.3603\n","Epoch [34/70] - Batch [3/3] - Test Loss: 2.3989\n","Epoch [34/70] - Train Loss: 0.3750, Test Loss: 0.9805\n","Epoch [35/70] - Batch [1/9] - Train Loss: 0.2416\n","Epoch [35/70] - Batch [2/9] - Train Loss: 0.1359\n","Epoch [35/70] - Batch [3/9] - Train Loss: 0.4118\n","Epoch [35/70] - Batch [4/9] - Train Loss: 0.3469\n","Epoch [35/70] - Batch [5/9] - Train Loss: 0.3398\n","Epoch [35/70] - Batch [6/9] - Train Loss: 0.2772\n","Epoch [35/70] - Batch [7/9] - Train Loss: 0.1030\n","Epoch [35/70] - Batch [8/9] - Train Loss: 0.1674\n","Epoch [35/70] - Batch [9/9] - Train Loss: 0.1932\n","Epoch [35/70] - Batch [1/3] - Test Loss: 0.4097\n","Epoch [35/70] - Batch [2/3] - Test Loss: 0.7721\n","Epoch [35/70] - Batch [3/3] - Test Loss: 0.3722\n","Epoch [35/70] - Train Loss: 0.2463, Test Loss: 0.5180\n","Epoch [36/70] - Batch [1/9] - Train Loss: 0.0507\n","Epoch [36/70] - Batch [2/9] - Train Loss: 0.3648\n","Epoch [36/70] - Batch [3/9] - Train Loss: 0.0464\n","Epoch [36/70] - Batch [4/9] - Train Loss: 0.0846\n","Epoch [36/70] - Batch [5/9] - Train Loss: 0.6952\n","Epoch [36/70] - Batch [6/9] - Train Loss: 0.0881\n","Epoch [36/70] - Batch [7/9] - Train Loss: 0.0831\n","Epoch [36/70] - Batch [8/9] - Train Loss: 0.0631\n","Epoch [36/70] - Batch [9/9] - Train Loss: 0.5341\n","Epoch [36/70] - Batch [1/3] - Test Loss: 0.6066\n","Epoch [36/70] - Batch [2/3] - Test Loss: 0.2199\n","Epoch [36/70] - Batch [3/3] - Test Loss: 4.4552\n","Epoch [36/70] - Train Loss: 0.2234, Test Loss: 1.7605\n","Epoch [37/70] - Batch [1/9] - Train Loss: 0.2403\n","Epoch [37/70] - Batch [2/9] - Train Loss: 0.0395\n","Epoch [37/70] - Batch [3/9] - Train Loss: 0.0528\n","Epoch [37/70] - Batch [4/9] - Train Loss: 0.0643\n","Epoch [37/70] - Batch [5/9] - Train Loss: 0.0975\n","Epoch [37/70] - Batch [6/9] - Train Loss: 0.0407\n","Epoch [37/70] - Batch [7/9] - Train Loss: 0.5466\n","Epoch [37/70] - Batch [8/9] - Train Loss: 0.0774\n","Epoch [37/70] - Batch [9/9] - Train Loss: 0.0701\n","Epoch [37/70] - Batch [1/3] - Test Loss: 0.3893\n","Epoch [37/70] - Batch [2/3] - Test Loss: 0.2643\n","Epoch [37/70] - Batch [3/3] - Test Loss: 0.0930\n","Epoch [37/70] - Train Loss: 0.1366, Test Loss: 0.2489\n","---<checkpoint saved>---0.24887060622374216\n","Epoch [38/70] - Batch [1/9] - Train Loss: 0.0594\n","Epoch [38/70] - Batch [2/9] - Train Loss: 0.0610\n","Epoch [38/70] - Batch [3/9] - Train Loss: 0.4254\n","Epoch [38/70] - Batch [4/9] - Train Loss: 0.0723\n","Epoch [38/70] - Batch [5/9] - Train Loss: 0.0788\n","Epoch [38/70] - Batch [6/9] - Train Loss: 0.3568\n","Epoch [38/70] - Batch [7/9] - Train Loss: 0.2858\n","Epoch [38/70] - Batch [8/9] - Train Loss: 0.0563\n","Epoch [38/70] - Batch [9/9] - Train Loss: 0.1168\n","Epoch [38/70] - Batch [1/3] - Test Loss: 0.3233\n","Epoch [38/70] - Batch [2/3] - Test Loss: 0.2585\n","Epoch [38/70] - Batch [3/3] - Test Loss: 2.7452\n","Epoch [38/70] - Train Loss: 0.1680, Test Loss: 1.1090\n","Epoch [39/70] - Batch [1/9] - Train Loss: 0.0675\n","Epoch [39/70] - Batch [2/9] - Train Loss: 0.0738\n","Epoch [39/70] - Batch [3/9] - Train Loss: 0.1081\n","Epoch [39/70] - Batch [4/9] - Train Loss: 0.9401\n","Epoch [39/70] - Batch [5/9] - Train Loss: 0.0299\n","Epoch [39/70] - Batch [6/9] - Train Loss: 0.1516\n","Epoch [39/70] - Batch [7/9] - Train Loss: 0.1320\n","Epoch [39/70] - Batch [8/9] - Train Loss: 0.0748\n","Epoch [39/70] - Batch [9/9] - Train Loss: 1.0959\n","Epoch [39/70] - Batch [1/3] - Test Loss: 0.4342\n","Epoch [39/70] - Batch [2/3] - Test Loss: 0.7032\n","Epoch [39/70] - Batch [3/3] - Test Loss: 0.0092\n","Epoch [39/70] - Train Loss: 0.2971, Test Loss: 0.3822\n","Epoch [40/70] - Batch [1/9] - Train Loss: 0.0672\n","Epoch [40/70] - Batch [2/9] - Train Loss: 0.1005\n","Epoch [40/70] - Batch [3/9] - Train Loss: 0.0947\n","Epoch [40/70] - Batch [4/9] - Train Loss: 0.0682\n","Epoch [40/70] - Batch [5/9] - Train Loss: 0.3617\n","Epoch [40/70] - Batch [6/9] - Train Loss: 0.2368\n","Epoch [40/70] - Batch [7/9] - Train Loss: 0.1325\n","Epoch [40/70] - Batch [8/9] - Train Loss: 0.0997\n","Epoch [40/70] - Batch [9/9] - Train Loss: 0.1032\n","Epoch [40/70] - Batch [1/3] - Test Loss: 0.2432\n","Epoch [40/70] - Batch [2/3] - Test Loss: 0.5729\n","Epoch [40/70] - Batch [3/3] - Test Loss: 0.5025\n","Epoch [40/70] - Train Loss: 0.1405, Test Loss: 0.4395\n","Epoch [41/70] - Batch [1/9] - Train Loss: 0.0740\n","Epoch [41/70] - Batch [2/9] - Train Loss: 0.3755\n","Epoch [41/70] - Batch [3/9] - Train Loss: 0.0729\n","Epoch [41/70] - Batch [4/9] - Train Loss: 0.2684\n","Epoch [41/70] - Batch [5/9] - Train Loss: 0.3078\n","Epoch [41/70] - Batch [6/9] - Train Loss: 0.0633\n","Epoch [41/70] - Batch [7/9] - Train Loss: 0.0956\n","Epoch [41/70] - Batch [8/9] - Train Loss: 0.2086\n","Epoch [41/70] - Batch [9/9] - Train Loss: 0.3776\n","Epoch [41/70] - Batch [1/3] - Test Loss: 0.3753\n","Epoch [41/70] - Batch [2/3] - Test Loss: 0.4418\n","Epoch [41/70] - Batch [3/3] - Test Loss: 0.6857\n","Epoch [41/70] - Train Loss: 0.2049, Test Loss: 0.5009\n","Epoch [42/70] - Batch [1/9] - Train Loss: 0.1317\n","Epoch [42/70] - Batch [2/9] - Train Loss: 0.3009\n","Epoch [42/70] - Batch [3/9] - Train Loss: 0.1581\n","Epoch [42/70] - Batch [4/9] - Train Loss: 0.1570\n","Epoch [42/70] - Batch [5/9] - Train Loss: 0.1512\n","Epoch [42/70] - Batch [6/9] - Train Loss: 0.2162\n","Epoch [42/70] - Batch [7/9] - Train Loss: 0.3508\n","Epoch [42/70] - Batch [8/9] - Train Loss: 0.2682\n","Epoch [42/70] - Batch [9/9] - Train Loss: 0.1644\n","Epoch [42/70] - Batch [1/3] - Test Loss: 0.4681\n","Epoch [42/70] - Batch [2/3] - Test Loss: 0.4774\n","Epoch [42/70] - Batch [3/3] - Test Loss: 0.5787\n","Epoch [42/70] - Train Loss: 0.2109, Test Loss: 0.5081\n","Epoch [43/70] - Batch [1/9] - Train Loss: 0.2457\n","Epoch [43/70] - Batch [2/9] - Train Loss: 0.0617\n","Epoch [43/70] - Batch [3/9] - Train Loss: 0.1642\n","Epoch [43/70] - Batch [4/9] - Train Loss: 0.1241\n","Epoch [43/70] - Batch [5/9] - Train Loss: 0.2375\n","Epoch [43/70] - Batch [6/9] - Train Loss: 0.7097\n","Epoch [43/70] - Batch [7/9] - Train Loss: 0.1134\n","Epoch [43/70] - Batch [8/9] - Train Loss: 0.0783\n","Epoch [43/70] - Batch [9/9] - Train Loss: 0.1045\n","Epoch [43/70] - Batch [1/3] - Test Loss: 0.4747\n","Epoch [43/70] - Batch [2/3] - Test Loss: 0.5397\n","Epoch [43/70] - Batch [3/3] - Test Loss: 0.1403\n","Epoch [43/70] - Train Loss: 0.2044, Test Loss: 0.3849\n","Epoch [44/70] - Batch [1/9] - Train Loss: 0.2247\n","Epoch [44/70] - Batch [2/9] - Train Loss: 0.6261\n","Epoch [44/70] - Batch [3/9] - Train Loss: 0.1895\n","Epoch [44/70] - Batch [4/9] - Train Loss: 0.1948\n","Epoch [44/70] - Batch [5/9] - Train Loss: 0.0906\n","Epoch [44/70] - Batch [6/9] - Train Loss: 0.1890\n","Epoch [44/70] - Batch [7/9] - Train Loss: 0.0867\n","Epoch [44/70] - Batch [8/9] - Train Loss: 0.2224\n","Epoch [44/70] - Batch [9/9] - Train Loss: 0.1803\n","Epoch [44/70] - Batch [1/3] - Test Loss: 0.4280\n","Epoch [44/70] - Batch [2/3] - Test Loss: 0.5229\n","Epoch [44/70] - Batch [3/3] - Test Loss: 8.6211\n","Epoch [44/70] - Train Loss: 0.2227, Test Loss: 3.1907\n","Epoch [45/70] - Batch [1/9] - Train Loss: 0.1161\n","Epoch [45/70] - Batch [2/9] - Train Loss: 0.2636\n","Epoch [45/70] - Batch [3/9] - Train Loss: 0.1309\n","Epoch [45/70] - Batch [4/9] - Train Loss: 0.0693\n","Epoch [45/70] - Batch [5/9] - Train Loss: 0.8308\n","Epoch [45/70] - Batch [6/9] - Train Loss: 0.1752\n","Epoch [45/70] - Batch [7/9] - Train Loss: 0.7300\n","Epoch [45/70] - Batch [8/9] - Train Loss: 0.5359\n","Epoch [45/70] - Batch [9/9] - Train Loss: 0.1100\n","Epoch [45/70] - Batch [1/3] - Test Loss: 0.7728\n","Epoch [45/70] - Batch [2/3] - Test Loss: 0.4227\n","Epoch [45/70] - Batch [3/3] - Test Loss: 0.3765\n","Epoch [45/70] - Train Loss: 0.3291, Test Loss: 0.5240\n","Epoch [46/70] - Batch [1/9] - Train Loss: 0.1913\n","Epoch [46/70] - Batch [2/9] - Train Loss: 0.7294\n","Epoch [46/70] - Batch [3/9] - Train Loss: 0.1956\n","Epoch [46/70] - Batch [4/9] - Train Loss: 0.7676\n","Epoch [46/70] - Batch [5/9] - Train Loss: 0.2527\n","Epoch [46/70] - Batch [6/9] - Train Loss: 0.1884\n","Epoch [46/70] - Batch [7/9] - Train Loss: 0.0641\n","Epoch [46/70] - Batch [8/9] - Train Loss: 0.0451\n","Epoch [46/70] - Batch [9/9] - Train Loss: 0.3808\n","Epoch [46/70] - Batch [1/3] - Test Loss: 0.2259\n","Epoch [46/70] - Batch [2/3] - Test Loss: 1.1346\n","Epoch [46/70] - Batch [3/3] - Test Loss: 0.2057\n","Epoch [46/70] - Train Loss: 0.3128, Test Loss: 0.5221\n","Epoch [47/70] - Batch [1/9] - Train Loss: 0.2432\n","Epoch [47/70] - Batch [2/9] - Train Loss: 0.0733\n","Epoch [47/70] - Batch [3/9] - Train Loss: 0.0378\n","Epoch [47/70] - Batch [4/9] - Train Loss: 0.1186\n","Epoch [47/70] - Batch [5/9] - Train Loss: 0.0606\n","Epoch [47/70] - Batch [6/9] - Train Loss: 0.1018\n","Epoch [47/70] - Batch [7/9] - Train Loss: 0.1098\n","Epoch [47/70] - Batch [8/9] - Train Loss: 0.0726\n","Epoch [47/70] - Batch [9/9] - Train Loss: 0.1868\n","Epoch [47/70] - Batch [1/3] - Test Loss: 0.2320\n","Epoch [47/70] - Batch [2/3] - Test Loss: 0.3734\n","Epoch [47/70] - Batch [3/3] - Test Loss: 12.2980\n","Epoch [47/70] - Train Loss: 0.1116, Test Loss: 4.3011\n","Epoch [48/70] - Batch [1/9] - Train Loss: 0.0426\n","Epoch [48/70] - Batch [2/9] - Train Loss: 0.2360\n","Epoch [48/70] - Batch [3/9] - Train Loss: 0.2517\n","Epoch [48/70] - Batch [4/9] - Train Loss: 0.1889\n","Epoch [48/70] - Batch [5/9] - Train Loss: 0.0575\n","Epoch [48/70] - Batch [6/9] - Train Loss: 0.1465\n","Epoch [48/70] - Batch [7/9] - Train Loss: 0.3309\n","Epoch [48/70] - Batch [8/9] - Train Loss: 0.2388\n","Epoch [48/70] - Batch [9/9] - Train Loss: 0.1401\n","Epoch [48/70] - Batch [1/3] - Test Loss: 0.4597\n","Epoch [48/70] - Batch [2/3] - Test Loss: 0.5980\n","Epoch [48/70] - Batch [3/3] - Test Loss: 0.7477\n","Epoch [48/70] - Train Loss: 0.1815, Test Loss: 0.6018\n","Epoch [49/70] - Batch [1/9] - Train Loss: 0.0436\n","Epoch [49/70] - Batch [2/9] - Train Loss: 0.0972\n","Epoch [49/70] - Batch [3/9] - Train Loss: 0.1122\n","Epoch [49/70] - Batch [4/9] - Train Loss: 0.0240\n","Epoch [49/70] - Batch [5/9] - Train Loss: 0.1346\n","Epoch [49/70] - Batch [6/9] - Train Loss: 0.2292\n","Epoch [49/70] - Batch [7/9] - Train Loss: 0.0752\n","Epoch [49/70] - Batch [8/9] - Train Loss: 0.0911\n","Epoch [49/70] - Batch [9/9] - Train Loss: 0.3471\n","Epoch [49/70] - Batch [1/3] - Test Loss: 0.3576\n","Epoch [49/70] - Batch [2/3] - Test Loss: 0.4852\n","Epoch [49/70] - Batch [3/3] - Test Loss: 0.4166\n","Epoch [49/70] - Train Loss: 0.1283, Test Loss: 0.4198\n","Epoch [50/70] - Batch [1/9] - Train Loss: 0.0585\n","Epoch [50/70] - Batch [2/9] - Train Loss: 0.3762\n","Epoch [50/70] - Batch [3/9] - Train Loss: 0.2293\n","Epoch [50/70] - Batch [4/9] - Train Loss: 0.1226\n","Epoch [50/70] - Batch [5/9] - Train Loss: 0.1261\n","Epoch [50/70] - Batch [6/9] - Train Loss: 0.0531\n","Epoch [50/70] - Batch [7/9] - Train Loss: 0.3196\n","Epoch [50/70] - Batch [8/9] - Train Loss: 0.1334\n","Epoch [50/70] - Batch [9/9] - Train Loss: 0.0599\n","Epoch [50/70] - Batch [1/3] - Test Loss: 0.6035\n","Epoch [50/70] - Batch [2/3] - Test Loss: 0.2836\n","Epoch [50/70] - Batch [3/3] - Test Loss: 0.2394\n","Epoch [50/70] - Train Loss: 0.1643, Test Loss: 0.3755\n","Epoch [51/70] - Batch [1/9] - Train Loss: 0.0743\n","Epoch [51/70] - Batch [2/9] - Train Loss: 0.1070\n","Epoch [51/70] - Batch [3/9] - Train Loss: 0.0930\n","Epoch [51/70] - Batch [4/9] - Train Loss: 0.2779\n","Epoch [51/70] - Batch [5/9] - Train Loss: 0.1131\n","Epoch [51/70] - Batch [6/9] - Train Loss: 0.1454\n","Epoch [51/70] - Batch [7/9] - Train Loss: 0.1201\n","Epoch [51/70] - Batch [8/9] - Train Loss: 0.0385\n","Epoch [51/70] - Batch [9/9] - Train Loss: 0.0853\n","Epoch [51/70] - Batch [1/3] - Test Loss: 0.2218\n","Epoch [51/70] - Batch [2/3] - Test Loss: 0.5585\n","Epoch [51/70] - Batch [3/3] - Test Loss: 0.0009\n","Epoch [51/70] - Train Loss: 0.1172, Test Loss: 0.2604\n","Epoch [52/70] - Batch [1/9] - Train Loss: 0.0559\n","Epoch [52/70] - Batch [2/9] - Train Loss: 0.1217\n","Epoch [52/70] - Batch [3/9] - Train Loss: 0.2457\n","Epoch [52/70] - Batch [4/9] - Train Loss: 0.3791\n","Epoch [52/70] - Batch [5/9] - Train Loss: 0.1029\n","Epoch [52/70] - Batch [6/9] - Train Loss: 0.0380\n","Epoch [52/70] - Batch [7/9] - Train Loss: 0.1487\n","Epoch [52/70] - Batch [8/9] - Train Loss: 0.2009\n","Epoch [52/70] - Batch [9/9] - Train Loss: 0.1488\n","Epoch [52/70] - Batch [1/3] - Test Loss: 0.4257\n","Epoch [52/70] - Batch [2/3] - Test Loss: 0.4068\n","Epoch [52/70] - Batch [3/3] - Test Loss: 1.9197\n","Epoch [52/70] - Train Loss: 0.1602, Test Loss: 0.9174\n","Epoch [53/70] - Batch [1/9] - Train Loss: 0.1904\n","Epoch [53/70] - Batch [2/9] - Train Loss: 0.0564\n","Epoch [53/70] - Batch [3/9] - Train Loss: 0.0438\n","Epoch [53/70] - Batch [4/9] - Train Loss: 0.0647\n","Epoch [53/70] - Batch [5/9] - Train Loss: 0.0702\n","Epoch [53/70] - Batch [6/9] - Train Loss: 0.3963\n","Epoch [53/70] - Batch [7/9] - Train Loss: 0.2231\n","Epoch [53/70] - Batch [8/9] - Train Loss: 0.2224\n","Epoch [53/70] - Batch [9/9] - Train Loss: 0.0846\n","Epoch [53/70] - Batch [1/3] - Test Loss: 0.3882\n","Epoch [53/70] - Batch [2/3] - Test Loss: 0.1509\n","Epoch [53/70] - Batch [3/3] - Test Loss: 0.0327\n","Epoch [53/70] - Train Loss: 0.1502, Test Loss: 0.1906\n","---<checkpoint saved>---0.19060024867455164\n","Epoch [54/70] - Batch [1/9] - Train Loss: 0.1340\n","Epoch [54/70] - Batch [2/9] - Train Loss: 0.0495\n","Epoch [54/70] - Batch [3/9] - Train Loss: 0.2237\n","Epoch [54/70] - Batch [4/9] - Train Loss: 0.1680\n","Epoch [54/70] - Batch [5/9] - Train Loss: 0.1066\n","Epoch [54/70] - Batch [6/9] - Train Loss: 0.0340\n","Epoch [54/70] - Batch [7/9] - Train Loss: 0.0256\n","Epoch [54/70] - Batch [8/9] - Train Loss: 0.0715\n","Epoch [54/70] - Batch [9/9] - Train Loss: 0.5128\n","Epoch [54/70] - Batch [1/3] - Test Loss: 1.0405\n","Epoch [54/70] - Batch [2/3] - Test Loss: 0.3763\n","Epoch [54/70] - Batch [3/3] - Test Loss: 0.0003\n","Epoch [54/70] - Train Loss: 0.1473, Test Loss: 0.4724\n","Epoch [55/70] - Batch [1/9] - Train Loss: 0.1020\n","Epoch [55/70] - Batch [2/9] - Train Loss: 0.0613\n","Epoch [55/70] - Batch [3/9] - Train Loss: 0.1479\n","Epoch [55/70] - Batch [4/9] - Train Loss: 0.3519\n","Epoch [55/70] - Batch [5/9] - Train Loss: 0.1587\n","Epoch [55/70] - Batch [6/9] - Train Loss: 0.0624\n","Epoch [55/70] - Batch [7/9] - Train Loss: 0.2234\n","Epoch [55/70] - Batch [8/9] - Train Loss: 0.3686\n","Epoch [55/70] - Batch [9/9] - Train Loss: 0.1187\n","Epoch [55/70] - Batch [1/3] - Test Loss: 0.2137\n","Epoch [55/70] - Batch [2/3] - Test Loss: 1.0202\n","Epoch [55/70] - Batch [3/3] - Test Loss: 0.0002\n","Epoch [55/70] - Train Loss: 0.1772, Test Loss: 0.4114\n","Epoch [56/70] - Batch [1/9] - Train Loss: 0.1861\n","Epoch [56/70] - Batch [2/9] - Train Loss: 0.1369\n","Epoch [56/70] - Batch [3/9] - Train Loss: 0.1005\n","Epoch [56/70] - Batch [4/9] - Train Loss: 0.0484\n","Epoch [56/70] - Batch [5/9] - Train Loss: 0.0356\n","Epoch [56/70] - Batch [6/9] - Train Loss: 0.2028\n","Epoch [56/70] - Batch [7/9] - Train Loss: 0.1205\n","Epoch [56/70] - Batch [8/9] - Train Loss: 0.2367\n","Epoch [56/70] - Batch [9/9] - Train Loss: 0.2710\n","Epoch [56/70] - Batch [1/3] - Test Loss: 0.5112\n","Epoch [56/70] - Batch [2/3] - Test Loss: 0.5268\n","Epoch [56/70] - Batch [3/3] - Test Loss: 0.2738\n","Epoch [56/70] - Train Loss: 0.1487, Test Loss: 0.4373\n","Epoch [57/70] - Batch [1/9] - Train Loss: 0.0641\n","Epoch [57/70] - Batch [2/9] - Train Loss: 0.0422\n","Epoch [57/70] - Batch [3/9] - Train Loss: 0.0459\n","Epoch [57/70] - Batch [4/9] - Train Loss: 0.1394\n","Epoch [57/70] - Batch [5/9] - Train Loss: 0.3120\n","Epoch [57/70] - Batch [6/9] - Train Loss: 0.1230\n","Epoch [57/70] - Batch [7/9] - Train Loss: 0.0467\n","Epoch [57/70] - Batch [8/9] - Train Loss: 0.0573\n","Epoch [57/70] - Batch [9/9] - Train Loss: 0.1027\n","Epoch [57/70] - Batch [1/3] - Test Loss: 0.4257\n","Epoch [57/70] - Batch [2/3] - Test Loss: 0.4082\n","Epoch [57/70] - Batch [3/3] - Test Loss: 1.2071\n","Epoch [57/70] - Train Loss: 0.1037, Test Loss: 0.6803\n","Epoch [58/70] - Batch [1/9] - Train Loss: 0.1047\n","Epoch [58/70] - Batch [2/9] - Train Loss: 0.1246\n","Epoch [58/70] - Batch [3/9] - Train Loss: 0.1079\n","Epoch [58/70] - Batch [4/9] - Train Loss: 0.0483\n","Epoch [58/70] - Batch [5/9] - Train Loss: 0.0570\n","Epoch [58/70] - Batch [6/9] - Train Loss: 0.0281\n","Epoch [58/70] - Batch [7/9] - Train Loss: 0.1171\n","Epoch [58/70] - Batch [8/9] - Train Loss: 0.1875\n","Epoch [58/70] - Batch [9/9] - Train Loss: 0.0680\n","Epoch [58/70] - Batch [1/3] - Test Loss: 0.1545\n","Epoch [58/70] - Batch [2/3] - Test Loss: 0.7709\n","Epoch [58/70] - Batch [3/3] - Test Loss: 0.0261\n","Epoch [58/70] - Train Loss: 0.0937, Test Loss: 0.3172\n","Epoch [59/70] - Batch [1/9] - Train Loss: 0.1193\n","Epoch [59/70] - Batch [2/9] - Train Loss: 0.4635\n","Epoch [59/70] - Batch [3/9] - Train Loss: 0.1549\n","Epoch [59/70] - Batch [4/9] - Train Loss: 0.0636\n","Epoch [59/70] - Batch [5/9] - Train Loss: 0.1359\n","Epoch [59/70] - Batch [6/9] - Train Loss: 0.0612\n","Epoch [59/70] - Batch [7/9] - Train Loss: 0.0303\n","Epoch [59/70] - Batch [8/9] - Train Loss: 0.0334\n","Epoch [59/70] - Batch [9/9] - Train Loss: 0.1013\n","Epoch [59/70] - Batch [1/3] - Test Loss: 0.8267\n","Epoch [59/70] - Batch [2/3] - Test Loss: 0.3571\n","Epoch [59/70] - Batch [3/3] - Test Loss: 1.4218\n","Epoch [59/70] - Train Loss: 0.1293, Test Loss: 0.8685\n","Epoch [60/70] - Batch [1/9] - Train Loss: 0.0458\n","Epoch [60/70] - Batch [2/9] - Train Loss: 0.0240\n","Epoch [60/70] - Batch [3/9] - Train Loss: 0.1347\n","Epoch [60/70] - Batch [4/9] - Train Loss: 0.0745\n","Epoch [60/70] - Batch [5/9] - Train Loss: 0.0827\n","Epoch [60/70] - Batch [6/9] - Train Loss: 0.0582\n","Epoch [60/70] - Batch [7/9] - Train Loss: 0.0944\n","Epoch [60/70] - Batch [8/9] - Train Loss: 0.0764\n","Epoch [60/70] - Batch [9/9] - Train Loss: 0.1082\n","Epoch [60/70] - Batch [1/3] - Test Loss: 0.8075\n","Epoch [60/70] - Batch [2/3] - Test Loss: 0.2987\n","Epoch [60/70] - Batch [3/3] - Test Loss: 0.5293\n","Epoch [60/70] - Train Loss: 0.0777, Test Loss: 0.5452\n","Epoch [61/70] - Batch [1/9] - Train Loss: 0.0478\n","Epoch [61/70] - Batch [2/9] - Train Loss: 0.0188\n","Epoch [61/70] - Batch [3/9] - Train Loss: 0.0796\n","Epoch [61/70] - Batch [4/9] - Train Loss: 0.0412\n","Epoch [61/70] - Batch [5/9] - Train Loss: 0.0568\n","Epoch [61/70] - Batch [6/9] - Train Loss: 0.1203\n","Epoch [61/70] - Batch [7/9] - Train Loss: 0.0862\n","Epoch [61/70] - Batch [8/9] - Train Loss: 0.0366\n","Epoch [61/70] - Batch [9/9] - Train Loss: 0.0091\n","Epoch [61/70] - Batch [1/3] - Test Loss: 0.6342\n","Epoch [61/70] - Batch [2/3] - Test Loss: 0.4268\n","Epoch [61/70] - Batch [3/3] - Test Loss: 0.0045\n","Epoch [61/70] - Train Loss: 0.0552, Test Loss: 0.3552\n","Epoch [62/70] - Batch [1/9] - Train Loss: 0.0229\n","Epoch [62/70] - Batch [2/9] - Train Loss: 0.0292\n","Epoch [62/70] - Batch [3/9] - Train Loss: 0.1306\n","Epoch [62/70] - Batch [4/9] - Train Loss: 0.0622\n","Epoch [62/70] - Batch [5/9] - Train Loss: 0.0512\n","Epoch [62/70] - Batch [6/9] - Train Loss: 0.0290\n","Epoch [62/70] - Batch [7/9] - Train Loss: 0.0535\n","Epoch [62/70] - Batch [8/9] - Train Loss: 0.1507\n","Epoch [62/70] - Batch [9/9] - Train Loss: 0.0821\n","Epoch [62/70] - Batch [1/3] - Test Loss: 0.4450\n","Epoch [62/70] - Batch [2/3] - Test Loss: 0.2930\n","Epoch [62/70] - Batch [3/3] - Test Loss: 0.0251\n","Epoch [62/70] - Train Loss: 0.0679, Test Loss: 0.2544\n","Epoch [63/70] - Batch [1/9] - Train Loss: 0.0213\n","Epoch [63/70] - Batch [2/9] - Train Loss: 0.0894\n","Epoch [63/70] - Batch [3/9] - Train Loss: 0.0355\n","Epoch [63/70] - Batch [4/9] - Train Loss: 0.0139\n","Epoch [63/70] - Batch [5/9] - Train Loss: 0.0383\n","Epoch [63/70] - Batch [6/9] - Train Loss: 0.0239\n","Epoch [63/70] - Batch [7/9] - Train Loss: 0.0296\n","Epoch [63/70] - Batch [8/9] - Train Loss: 0.0190\n","Epoch [63/70] - Batch [9/9] - Train Loss: 0.0492\n","Epoch [63/70] - Batch [1/3] - Test Loss: 0.3637\n","Epoch [63/70] - Batch [2/3] - Test Loss: 0.1280\n","Epoch [63/70] - Batch [3/3] - Test Loss: 1.6277\n","Epoch [63/70] - Train Loss: 0.0356, Test Loss: 0.7065\n","Epoch [64/70] - Batch [1/9] - Train Loss: 0.0293\n","Epoch [64/70] - Batch [2/9] - Train Loss: 0.0480\n","Epoch [64/70] - Batch [3/9] - Train Loss: 0.0164\n","Epoch [64/70] - Batch [4/9] - Train Loss: 0.0184\n","Epoch [64/70] - Batch [5/9] - Train Loss: 0.0111\n","Epoch [64/70] - Batch [6/9] - Train Loss: 0.0853\n","Epoch [64/70] - Batch [7/9] - Train Loss: 0.0657\n","Epoch [64/70] - Batch [8/9] - Train Loss: 0.0289\n","Epoch [64/70] - Batch [9/9] - Train Loss: 0.0262\n","Epoch [64/70] - Batch [1/3] - Test Loss: 0.3884\n","Epoch [64/70] - Batch [2/3] - Test Loss: 0.1835\n","Epoch [64/70] - Batch [3/3] - Test Loss: 0.0121\n","Epoch [64/70] - Train Loss: 0.0366, Test Loss: 0.1947\n","Epoch [65/70] - Batch [1/9] - Train Loss: 0.0530\n","Epoch [65/70] - Batch [2/9] - Train Loss: 0.1070\n","Epoch [65/70] - Batch [3/9] - Train Loss: 0.0194\n","Epoch [65/70] - Batch [4/9] - Train Loss: 0.1298\n","Epoch [65/70] - Batch [5/9] - Train Loss: 0.1257\n","Epoch [65/70] - Batch [6/9] - Train Loss: 0.0345\n","Epoch [65/70] - Batch [7/9] - Train Loss: 0.0849\n","Epoch [65/70] - Batch [8/9] - Train Loss: 0.0598\n","Epoch [65/70] - Batch [9/9] - Train Loss: 0.0579\n","Epoch [65/70] - Batch [1/3] - Test Loss: 0.1869\n","Epoch [65/70] - Batch [2/3] - Test Loss: 0.3362\n","Epoch [65/70] - Batch [3/3] - Test Loss: 0.1019\n","Epoch [65/70] - Train Loss: 0.0747, Test Loss: 0.2083\n","Epoch [66/70] - Batch [1/9] - Train Loss: 0.0833\n","Epoch [66/70] - Batch [2/9] - Train Loss: 0.1618\n","Epoch [66/70] - Batch [3/9] - Train Loss: 0.1126\n","Epoch [66/70] - Batch [4/9] - Train Loss: 0.0895\n","Epoch [66/70] - Batch [5/9] - Train Loss: 0.0311\n","Epoch [66/70] - Batch [6/9] - Train Loss: 0.0217\n","Epoch [66/70] - Batch [7/9] - Train Loss: 0.0667\n","Epoch [66/70] - Batch [8/9] - Train Loss: 0.0337\n","Epoch [66/70] - Batch [9/9] - Train Loss: 0.0574\n","Epoch [66/70] - Batch [1/3] - Test Loss: 0.1141\n","Epoch [66/70] - Batch [2/3] - Test Loss: 0.7531\n","Epoch [66/70] - Batch [3/3] - Test Loss: 0.5602\n","Epoch [66/70] - Train Loss: 0.0731, Test Loss: 0.4758\n","Epoch [67/70] - Batch [1/9] - Train Loss: 0.0374\n","Epoch [67/70] - Batch [2/9] - Train Loss: 0.0296\n","Epoch [67/70] - Batch [3/9] - Train Loss: 0.0405\n","Epoch [67/70] - Batch [4/9] - Train Loss: 0.0509\n","Epoch [67/70] - Batch [5/9] - Train Loss: 0.0271\n","Epoch [67/70] - Batch [6/9] - Train Loss: 0.0281\n","Epoch [67/70] - Batch [7/9] - Train Loss: 0.0188\n","Epoch [67/70] - Batch [8/9] - Train Loss: 0.0216\n","Epoch [67/70] - Batch [9/9] - Train Loss: 0.0306\n","Epoch [67/70] - Batch [1/3] - Test Loss: 0.0909\n","Epoch [67/70] - Batch [2/3] - Test Loss: 0.4811\n","Epoch [67/70] - Batch [3/3] - Test Loss: 2.6185\n","Epoch [67/70] - Train Loss: 0.0316, Test Loss: 1.0635\n","Epoch [68/70] - Batch [1/9] - Train Loss: 0.0272\n","Epoch [68/70] - Batch [2/9] - Train Loss: 0.0513\n","Epoch [68/70] - Batch [3/9] - Train Loss: 0.0308\n","Epoch [68/70] - Batch [4/9] - Train Loss: 0.0209\n","Epoch [68/70] - Batch [5/9] - Train Loss: 0.0364\n","Epoch [68/70] - Batch [6/9] - Train Loss: 0.1338\n","Epoch [68/70] - Batch [7/9] - Train Loss: 0.0363\n","Epoch [68/70] - Batch [8/9] - Train Loss: 0.0452\n","Epoch [68/70] - Batch [9/9] - Train Loss: 0.2069\n","Epoch [68/70] - Batch [1/3] - Test Loss: 0.1987\n","Epoch [68/70] - Batch [2/3] - Test Loss: 0.2904\n","Epoch [68/70] - Batch [3/3] - Test Loss: 0.2152\n","Epoch [68/70] - Train Loss: 0.0654, Test Loss: 0.2348\n","Epoch [69/70] - Batch [1/9] - Train Loss: 0.0750\n","Epoch [69/70] - Batch [2/9] - Train Loss: 0.0074\n","Epoch [69/70] - Batch [3/9] - Train Loss: 0.0335\n","Epoch [69/70] - Batch [4/9] - Train Loss: 0.0255\n","Epoch [69/70] - Batch [5/9] - Train Loss: 0.1039\n","Epoch [69/70] - Batch [6/9] - Train Loss: 0.1161\n","Epoch [69/70] - Batch [7/9] - Train Loss: 0.0952\n","Epoch [69/70] - Batch [8/9] - Train Loss: 0.0208\n","Epoch [69/70] - Batch [9/9] - Train Loss: 0.0711\n","Epoch [69/70] - Batch [1/3] - Test Loss: 0.1497\n","Epoch [69/70] - Batch [2/3] - Test Loss: 0.2270\n","Epoch [69/70] - Batch [3/3] - Test Loss: 0.1160\n","Epoch [69/70] - Train Loss: 0.0609, Test Loss: 0.1642\n","---<checkpoint saved>---0.16422888139883676\n","Epoch [70/70] - Batch [1/9] - Train Loss: 0.0790\n","Epoch [70/70] - Batch [2/9] - Train Loss: 0.0511\n","Epoch [70/70] - Batch [3/9] - Train Loss: 0.0337\n","Epoch [70/70] - Batch [4/9] - Train Loss: 0.0275\n","Epoch [70/70] - Batch [5/9] - Train Loss: 0.0260\n","Epoch [70/70] - Batch [6/9] - Train Loss: 0.0236\n","Epoch [70/70] - Batch [7/9] - Train Loss: 0.0262\n","Epoch [70/70] - Batch [8/9] - Train Loss: 0.1902\n","Epoch [70/70] - Batch [9/9] - Train Loss: 0.1865\n","Epoch [70/70] - Batch [1/3] - Test Loss: 0.2223\n","Epoch [70/70] - Batch [2/3] - Test Loss: 0.1838\n","Epoch [70/70] - Batch [3/3] - Test Loss: 0.3180\n","Epoch [70/70] - Train Loss: 0.0715, Test Loss: 0.2414\n"]}],"source":["NUM_EPOCHS = 70\n","CHECKPOINT_PATH = 'checkpoint.pth'\n","best_loss = 1e9\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    train_loss = 0.0\n","\n","    # Initialize counters for debugging\n","    total_train_batches = len(train_loader)\n","    train_batch_count = 0\n","\n","    for images, labels in iter(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = F.mse_loss(outputs, labels)\n","        train_loss += float(loss)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print detailed training information for each batch\n","        train_batch_count += 1\n","        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Batch [{train_batch_count}/{total_train_batches}] - Train Loss: {loss:.4f}\")\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    test_loss = 0.0\n","\n","    # Initialize counters for debugging\n","    total_test_batches = len(test_loader)\n","    test_batch_count = 0\n","\n","    for images, labels in iter(test_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        loss = F.mse_loss(outputs, labels)\n","        test_loss += float(loss)\n","\n","        # Print detailed testing information for each batch\n","        test_batch_count += 1\n","        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Batch [{test_batch_count}/{total_test_batches}] - Test Loss: {loss:.4f}\")\n","\n","    test_loss /= len(test_loader)\n","\n","    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}] - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n","\n","    if test_loss < best_loss:\n","        torch.save(model.state_dict(), CHECKPOINT_PATH)\n","        best_loss = test_loss\n","        print(f\"---<checkpoint saved>---{best_loss}\")\n"]},{"cell_type":"markdown","metadata":{"id":"q_h0o4slNjFe"},"source":["Once the model is trained, it will generate ``checkpoint.pth`` file will save the model with lowest loss function. And this model can be used for prediction."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}
