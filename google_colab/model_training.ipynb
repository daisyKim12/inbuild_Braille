{"cells":[{"cell_type":"markdown","metadata":{"id":"B7tAP-7aNjFL"},"source":["# Side Walk Guide - Train Model\n","\n","In this notebook we will train a neural network to take an input image, and output x values corresponding to a target.\n","\n","We will be using PyTorch deep learning framework to train ResNet18 neural network architecture model for road follower application."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"z7h3sSeaNjFR","executionInfo":{"status":"ok","timestamp":1701107448636,"user_tz":-540,"elapsed":6,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}}},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import glob\n","import PIL.Image\n","import os\n","import numpy as np\n","import torch.utils.data\n"]},{"cell_type":"markdown","metadata":{"id":"G36EvUvhNjFT"},"source":["### Download and extract data\n","\n","Before you start, you should upload the ``training_dataset.zip`` file that you created in the ``data_collection.ipynb``\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OTUV72qSNjFU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701107452701,"user_tz":-540,"elapsed":4068,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}},"outputId":"72d4baab-30c4-4242-a8d4-783b65d736b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  training_data2.zip\n","   creating: training_data2/\n","replace __MACOSX/._training_data2? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["!unzip training_data2.zip\n","!mv training_data1 training_dataset\n","# !unzip training_data2\n","# !mv training_data2/* training_dataset"]},{"cell_type":"markdown","metadata":{"id":"KJBNVsLdNjFV"},"source":["### Create Dataset Instance\n","\n","Here we create a custom ``torch.utils.data.Dataset`` implementation, which implements the ``__len__`` and ``__getitem__`` functions.  This class\n","is responsible for loading images and parsing the x, y values from the image filenames.  Because we implement the ``torch.utils.data.Dataset`` class,\n","we can use all of the torch data utilities"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"S1CHn9g9NjFW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701107453368,"user_tz":-540,"elapsed":671,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}},"outputId":"6bfbcc80-eff3-4060-cbac-f5a9fb8b0e0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample 1 - Image Shape: torch.Size([3, 224, 224]), x: -1.649999976158142\n","Sample 2 - Image Shape: torch.Size([3, 224, 224]), x: -0.23333333432674408\n","Sample 3 - Image Shape: torch.Size([3, 224, 224]), x: -4.300000190734863\n","Sample 4 - Image Shape: torch.Size([3, 224, 224]), x: 1.4166666269302368\n","Sample 5 - Image Shape: torch.Size([3, 224, 224]), x: -2.6500000953674316\n","Total number of samples in the dataset: 201\n"]}],"source":["def get_x(file_name):\n","    \"\"\"Gets the x value from the image filename\"\"\"\n","    token = file_name.split(\"-\")\n","    return (float(int(token[1].split(\".\")[0]) - 300.0) * 5 / 300.0)\n","\n","class XLableAugmentImageTrainData(torch.utils.data.Dataset):\n","\n","    def __init__(self, directory, random_hflips=False):\n","        self.directory = directory\n","        self.random_hflips = random_hflips\n","        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n","        self.augmentation = transforms.Compose([\n","            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),\n","            transforms.RandomRotation(degrees=30),\n","            transforms.RandomVerticalFlip(p=0.5),\n","            transforms.RandomApply([transforms.ColorJitter(0.5, 0.5, 0.5, 0.5)], p=0.5),\n","            transforms.RandomApply([transforms.Grayscale(num_output_channels=3)], p=0.2),\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","\n","        image = PIL.Image.open(image_path)\n","        x = float(get_x(os.path.basename(image_path)))\n","\n","        if self.random_hflips and float(np.random.rand(1)) > 0.5:\n","            image = transforms.functional.hflip(image)\n","            x = -x\n","\n","        # Apply augmentations\n","        image = self.augmentation(image)\n","\n","        image = transforms.functional.resize(image, (224, 224))\n","        image = transforms.functional.to_tensor(image)\n","        image = image.numpy()[::-1].copy()\n","        image = torch.from_numpy(image)\n","        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","\n","        return image, torch.tensor([x]).float()\n","\n","# Example usage\n","dataset = XLableAugmentImageTrainData('training_dataset', random_hflips=True)\n","\n","for i in range(5):\n","    image, x = dataset[i]\n","    print(f\"Sample {i+1} - Image Shape: {image.shape}, x: {x.item()}\")\n","total_samples = len(dataset)\n","print(f\"Total number of samples in the dataset: {total_samples}\")\n"]},{"cell_type":"markdown","metadata":{"id":"BVKfkDLkNjFY"},"source":["### Split dataset into train and test sets\n","Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 80%-20%. The test set will be used to verify the accuracy of the model we train."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jlzdcun6NjFZ","executionInfo":{"status":"ok","timestamp":1701107453369,"user_tz":-540,"elapsed":15,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}}},"outputs":[],"source":["test_percent = 0.2\n","num_test = int(test_percent * len(dataset))\n","train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"]},{"cell_type":"markdown","metadata":{"id":"wiqblvTDNjFa"},"source":["### Create data loaders to load data in batches\n","\n","We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"tp7EW9KQNjFb","executionInfo":{"status":"ok","timestamp":1701107453369,"user_tz":-540,"elapsed":14,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}}},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    shuffle=True,\n","    num_workers=2\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset,\n","    batch_size=16,\n","    shuffle=True,\n","    num_workers=2\n",")"]},{"cell_type":"markdown","metadata":{"id":"4CAblercNjFc"},"source":["### Define Neural Network Model\n","\n","We use ResNet-18 model available on PyTorch TorchVision.\n","\n","In a process called transfer learning, we can repurpose a pre-trained model (trained on millions of images) for a new task that has possibly much less data available.\n","\n","\n","More details on ResNet-18 : https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n","\n","More Details on Transfer Learning: https://www.youtube.com/watch?v=yofjFQddwHE"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"T2HzY-JuNjFc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701107464744,"user_tz":-540,"elapsed":960,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}},"outputId":"cfa72ca8-a9ed-4644-a4e5-6b59bcff82ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 123MB/s]\n"]}],"source":["model = models.resnet18(pretrained=True)"]},{"cell_type":"markdown","metadata":{"id":"GTu0v3l3NjFd"},"source":["ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 1\n","\n","Finally, we transfer our model for execution on the GPU"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"TYnQRd5uNjFd","executionInfo":{"status":"ok","timestamp":1701107468754,"user_tz":-540,"elapsed":468,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}}},"outputs":[],"source":["model.fc = torch.nn.Linear(512, 1)\n","device = torch.device('cuda')\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"ap7E4lriNjFe"},"source":["### Train Regression:\n","\n","We train for 50 epochs and save best model if the loss is reduced."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GOxdxANCNjFe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701107952856,"user_tz":-540,"elapsed":482736,"user":{"displayName":"김석진 (데이지)","userId":"03908017394417747553"}},"outputId":"7209f192-c401-40ac-da1e-bc19de35e8f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50] - Batch [1/11] - Train Loss: 7.9862\n","Epoch [1/50] - Batch [2/11] - Train Loss: 5.6437\n","Epoch [1/50] - Batch [3/11] - Train Loss: 7.7024\n","Epoch [1/50] - Batch [4/11] - Train Loss: 8.2697\n","Epoch [1/50] - Batch [5/11] - Train Loss: 5.4610\n","Epoch [1/50] - Batch [6/11] - Train Loss: 5.9512\n","Epoch [1/50] - Batch [7/11] - Train Loss: 6.5032\n","Epoch [1/50] - Batch [8/11] - Train Loss: 6.0601\n","Epoch [1/50] - Batch [9/11] - Train Loss: 4.3758\n","Epoch [1/50] - Batch [10/11] - Train Loss: 6.8699\n","Epoch [1/50] - Batch [11/11] - Train Loss: 1.0570\n","Epoch [1/50] - Batch [1/3] - Test Loss: 59.8005\n","Epoch [1/50] - Batch [2/3] - Test Loss: 49.9575\n","Epoch [1/50] - Batch [3/3] - Test Loss: 42.4331\n","Epoch [1/50] - Train Loss: 5.9891, Test Loss: 50.7304\n","---<checkpoint saved>---50.73037083943685\n","Epoch [2/50] - Batch [1/11] - Train Loss: 2.7954\n","Epoch [2/50] - Batch [2/11] - Train Loss: 5.4013\n","Epoch [2/50] - Batch [3/11] - Train Loss: 3.0810\n","Epoch [2/50] - Batch [4/11] - Train Loss: 3.8367\n","Epoch [2/50] - Batch [5/11] - Train Loss: 2.7021\n","Epoch [2/50] - Batch [6/11] - Train Loss: 3.6343\n","Epoch [2/50] - Batch [7/11] - Train Loss: 2.8837\n","Epoch [2/50] - Batch [8/11] - Train Loss: 2.4339\n","Epoch [2/50] - Batch [9/11] - Train Loss: 3.1128\n","Epoch [2/50] - Batch [10/11] - Train Loss: 1.6863\n","Epoch [2/50] - Batch [11/11] - Train Loss: 1.1859\n","Epoch [2/50] - Batch [1/3] - Test Loss: 11.1621\n","Epoch [2/50] - Batch [2/3] - Test Loss: 10.8999\n","Epoch [2/50] - Batch [3/3] - Test Loss: 15.2085\n","Epoch [2/50] - Train Loss: 2.9776, Test Loss: 12.4235\n","---<checkpoint saved>---12.423514048258463\n","Epoch [3/50] - Batch [1/11] - Train Loss: 1.7189\n","Epoch [3/50] - Batch [2/11] - Train Loss: 2.5656\n","Epoch [3/50] - Batch [3/11] - Train Loss: 1.6092\n","Epoch [3/50] - Batch [4/11] - Train Loss: 2.3441\n","Epoch [3/50] - Batch [5/11] - Train Loss: 1.7580\n","Epoch [3/50] - Batch [6/11] - Train Loss: 1.3102\n","Epoch [3/50] - Batch [7/11] - Train Loss: 1.6921\n","Epoch [3/50] - Batch [8/11] - Train Loss: 2.4323\n","Epoch [3/50] - Batch [9/11] - Train Loss: 1.2827\n","Epoch [3/50] - Batch [10/11] - Train Loss: 0.9760\n","Epoch [3/50] - Batch [11/11] - Train Loss: 19.1392\n","Epoch [3/50] - Batch [1/3] - Test Loss: 1.6430\n","Epoch [3/50] - Batch [2/3] - Test Loss: 5.9053\n","Epoch [3/50] - Batch [3/3] - Test Loss: 2.4514\n","Epoch [3/50] - Train Loss: 3.3480, Test Loss: 3.3332\n","---<checkpoint saved>---3.3332364559173584\n","Epoch [4/50] - Batch [1/11] - Train Loss: 0.6533\n","Epoch [4/50] - Batch [2/11] - Train Loss: 1.4445\n","Epoch [4/50] - Batch [3/11] - Train Loss: 4.6016\n","Epoch [4/50] - Batch [4/11] - Train Loss: 1.2991\n","Epoch [4/50] - Batch [5/11] - Train Loss: 1.1126\n","Epoch [4/50] - Batch [6/11] - Train Loss: 3.7171\n","Epoch [4/50] - Batch [7/11] - Train Loss: 2.0295\n","Epoch [4/50] - Batch [8/11] - Train Loss: 1.0643\n","Epoch [4/50] - Batch [9/11] - Train Loss: 1.5827\n","Epoch [4/50] - Batch [10/11] - Train Loss: 1.3482\n","Epoch [4/50] - Batch [11/11] - Train Loss: 0.4887\n","Epoch [4/50] - Batch [1/3] - Test Loss: 2.7677\n","Epoch [4/50] - Batch [2/3] - Test Loss: 3.7148\n","Epoch [4/50] - Batch [3/3] - Test Loss: 0.9012\n","Epoch [4/50] - Train Loss: 1.7583, Test Loss: 2.4612\n","---<checkpoint saved>---2.461234450340271\n","Epoch [5/50] - Batch [1/11] - Train Loss: 2.3175\n","Epoch [5/50] - Batch [2/11] - Train Loss: 0.4988\n","Epoch [5/50] - Batch [3/11] - Train Loss: 2.5351\n","Epoch [5/50] - Batch [4/11] - Train Loss: 1.0308\n","Epoch [5/50] - Batch [5/11] - Train Loss: 2.8812\n","Epoch [5/50] - Batch [6/11] - Train Loss: 0.7494\n","Epoch [5/50] - Batch [7/11] - Train Loss: 2.0486\n","Epoch [5/50] - Batch [8/11] - Train Loss: 1.5569\n","Epoch [5/50] - Batch [9/11] - Train Loss: 1.3300\n","Epoch [5/50] - Batch [10/11] - Train Loss: 1.9313\n","Epoch [5/50] - Batch [11/11] - Train Loss: 0.0004\n","Epoch [5/50] - Batch [1/3] - Test Loss: 1.2436\n","Epoch [5/50] - Batch [2/3] - Test Loss: 2.4099\n","Epoch [5/50] - Batch [3/3] - Test Loss: 3.4422\n","Epoch [5/50] - Train Loss: 1.5345, Test Loss: 2.3652\n","---<checkpoint saved>---2.3652267456054688\n","Epoch [6/50] - Batch [1/11] - Train Loss: 1.6696\n","Epoch [6/50] - Batch [2/11] - Train Loss: 3.6366\n","Epoch [6/50] - Batch [3/11] - Train Loss: 2.9459\n","Epoch [6/50] - Batch [4/11] - Train Loss: 1.2635\n","Epoch [6/50] - Batch [5/11] - Train Loss: 1.2975\n","Epoch [6/50] - Batch [6/11] - Train Loss: 1.6092\n","Epoch [6/50] - Batch [7/11] - Train Loss: 1.5073\n","Epoch [6/50] - Batch [8/11] - Train Loss: 2.1018\n","Epoch [6/50] - Batch [9/11] - Train Loss: 1.8674\n","Epoch [6/50] - Batch [10/11] - Train Loss: 1.6610\n","Epoch [6/50] - Batch [11/11] - Train Loss: 3.0461\n","Epoch [6/50] - Batch [1/3] - Test Loss: 0.9926\n","Epoch [6/50] - Batch [2/3] - Test Loss: 1.8188\n","Epoch [6/50] - Batch [3/3] - Test Loss: 1.9166\n","Epoch [6/50] - Train Loss: 2.0551, Test Loss: 1.5760\n","---<checkpoint saved>---1.5759903192520142\n","Epoch [7/50] - Batch [1/11] - Train Loss: 1.6662\n","Epoch [7/50] - Batch [2/11] - Train Loss: 1.1073\n","Epoch [7/50] - Batch [3/11] - Train Loss: 1.3826\n","Epoch [7/50] - Batch [4/11] - Train Loss: 3.5303\n","Epoch [7/50] - Batch [5/11] - Train Loss: 1.3462\n","Epoch [7/50] - Batch [6/11] - Train Loss: 0.8009\n","Epoch [7/50] - Batch [7/11] - Train Loss: 0.9474\n","Epoch [7/50] - Batch [8/11] - Train Loss: 0.7689\n","Epoch [7/50] - Batch [9/11] - Train Loss: 1.8461\n","Epoch [7/50] - Batch [10/11] - Train Loss: 1.7439\n","Epoch [7/50] - Batch [11/11] - Train Loss: 0.5138\n","Epoch [7/50] - Batch [1/3] - Test Loss: 1.4112\n","Epoch [7/50] - Batch [2/3] - Test Loss: 0.8860\n","Epoch [7/50] - Batch [3/3] - Test Loss: 0.5202\n","Epoch [7/50] - Train Loss: 1.4231, Test Loss: 0.9391\n","---<checkpoint saved>---0.9391307632128397\n","Epoch [8/50] - Batch [1/11] - Train Loss: 0.7336\n","Epoch [8/50] - Batch [2/11] - Train Loss: 7.8698\n","Epoch [8/50] - Batch [3/11] - Train Loss: 0.9280\n","Epoch [8/50] - Batch [4/11] - Train Loss: 1.4305\n","Epoch [8/50] - Batch [5/11] - Train Loss: 2.0791\n","Epoch [8/50] - Batch [6/11] - Train Loss: 2.0343\n","Epoch [8/50] - Batch [7/11] - Train Loss: 2.5089\n","Epoch [8/50] - Batch [8/11] - Train Loss: 1.2914\n","Epoch [8/50] - Batch [9/11] - Train Loss: 4.2159\n","Epoch [8/50] - Batch [10/11] - Train Loss: 1.2664\n","Epoch [8/50] - Batch [11/11] - Train Loss: 2.0791\n","Epoch [8/50] - Batch [1/3] - Test Loss: 2.2396\n","Epoch [8/50] - Batch [2/3] - Test Loss: 1.5633\n","Epoch [8/50] - Batch [3/3] - Test Loss: 0.5741\n","Epoch [8/50] - Train Loss: 2.4034, Test Loss: 1.4590\n","Epoch [9/50] - Batch [1/11] - Train Loss: 2.9512\n","Epoch [9/50] - Batch [2/11] - Train Loss: 1.9483\n","Epoch [9/50] - Batch [3/11] - Train Loss: 2.2994\n","Epoch [9/50] - Batch [4/11] - Train Loss: 2.8941\n","Epoch [9/50] - Batch [5/11] - Train Loss: 0.5863\n","Epoch [9/50] - Batch [6/11] - Train Loss: 2.1855\n","Epoch [9/50] - Batch [7/11] - Train Loss: 1.4400\n","Epoch [9/50] - Batch [8/11] - Train Loss: 0.8095\n","Epoch [9/50] - Batch [9/11] - Train Loss: 0.8495\n","Epoch [9/50] - Batch [10/11] - Train Loss: 1.2467\n","Epoch [9/50] - Batch [11/11] - Train Loss: 0.7215\n","Epoch [9/50] - Batch [1/3] - Test Loss: 0.7304\n","Epoch [9/50] - Batch [2/3] - Test Loss: 1.0237\n","Epoch [9/50] - Batch [3/3] - Test Loss: 2.7120\n","Epoch [9/50] - Train Loss: 1.6302, Test Loss: 1.4887\n","Epoch [10/50] - Batch [1/11] - Train Loss: 0.5925\n","Epoch [10/50] - Batch [2/11] - Train Loss: 0.6810\n","Epoch [10/50] - Batch [3/11] - Train Loss: 1.2534\n","Epoch [10/50] - Batch [4/11] - Train Loss: 0.8800\n","Epoch [10/50] - Batch [5/11] - Train Loss: 1.0527\n","Epoch [10/50] - Batch [6/11] - Train Loss: 1.1294\n","Epoch [10/50] - Batch [7/11] - Train Loss: 0.9781\n","Epoch [10/50] - Batch [8/11] - Train Loss: 0.6519\n","Epoch [10/50] - Batch [9/11] - Train Loss: 1.0726\n","Epoch [10/50] - Batch [10/11] - Train Loss: 4.8651\n","Epoch [10/50] - Batch [11/11] - Train Loss: 1.4545\n","Epoch [10/50] - Batch [1/3] - Test Loss: 0.7527\n","Epoch [10/50] - Batch [2/3] - Test Loss: 0.5300\n","Epoch [10/50] - Batch [3/3] - Test Loss: 2.2302\n","Epoch [10/50] - Train Loss: 1.3283, Test Loss: 1.1710\n","Epoch [11/50] - Batch [1/11] - Train Loss: 1.5867\n","Epoch [11/50] - Batch [2/11] - Train Loss: 1.2234\n","Epoch [11/50] - Batch [3/11] - Train Loss: 0.9206\n","Epoch [11/50] - Batch [4/11] - Train Loss: 1.2445\n","Epoch [11/50] - Batch [5/11] - Train Loss: 2.0923\n","Epoch [11/50] - Batch [6/11] - Train Loss: 2.2100\n","Epoch [11/50] - Batch [7/11] - Train Loss: 1.0937\n","Epoch [11/50] - Batch [8/11] - Train Loss: 0.8455\n","Epoch [11/50] - Batch [9/11] - Train Loss: 0.7215\n","Epoch [11/50] - Batch [10/11] - Train Loss: 0.9956\n","Epoch [11/50] - Batch [11/11] - Train Loss: 17.6091\n","Epoch [11/50] - Batch [1/3] - Test Loss: 0.7832\n","Epoch [11/50] - Batch [2/3] - Test Loss: 0.9841\n","Epoch [11/50] - Batch [3/3] - Test Loss: 1.3997\n","Epoch [11/50] - Train Loss: 2.7766, Test Loss: 1.0557\n","Epoch [12/50] - Batch [1/11] - Train Loss: 1.5930\n","Epoch [12/50] - Batch [2/11] - Train Loss: 0.5712\n","Epoch [12/50] - Batch [3/11] - Train Loss: 0.8044\n","Epoch [12/50] - Batch [4/11] - Train Loss: 1.3407\n","Epoch [12/50] - Batch [5/11] - Train Loss: 3.4825\n","Epoch [12/50] - Batch [6/11] - Train Loss: 2.2116\n","Epoch [12/50] - Batch [7/11] - Train Loss: 1.2378\n","Epoch [12/50] - Batch [8/11] - Train Loss: 1.2230\n","Epoch [12/50] - Batch [9/11] - Train Loss: 1.4198\n","Epoch [12/50] - Batch [10/11] - Train Loss: 0.8349\n","Epoch [12/50] - Batch [11/11] - Train Loss: 8.6954\n","Epoch [12/50] - Batch [1/3] - Test Loss: 0.8437\n","Epoch [12/50] - Batch [2/3] - Test Loss: 1.4549\n","Epoch [12/50] - Batch [3/3] - Test Loss: 0.6030\n","Epoch [12/50] - Train Loss: 2.1286, Test Loss: 0.9672\n","Epoch [13/50] - Batch [1/11] - Train Loss: 0.8175\n","Epoch [13/50] - Batch [2/11] - Train Loss: 1.7524\n","Epoch [13/50] - Batch [3/11] - Train Loss: 0.9095\n","Epoch [13/50] - Batch [4/11] - Train Loss: 1.5677\n","Epoch [13/50] - Batch [5/11] - Train Loss: 1.2132\n","Epoch [13/50] - Batch [6/11] - Train Loss: 2.8684\n","Epoch [13/50] - Batch [7/11] - Train Loss: 3.8820\n","Epoch [13/50] - Batch [8/11] - Train Loss: 3.4536\n","Epoch [13/50] - Batch [9/11] - Train Loss: 0.9665\n","Epoch [13/50] - Batch [10/11] - Train Loss: 3.4416\n","Epoch [13/50] - Batch [11/11] - Train Loss: 0.0037\n","Epoch [13/50] - Batch [1/3] - Test Loss: 0.8563\n","Epoch [13/50] - Batch [2/3] - Test Loss: 0.9296\n","Epoch [13/50] - Batch [3/3] - Test Loss: 0.8932\n","Epoch [13/50] - Train Loss: 1.8978, Test Loss: 0.8931\n","---<checkpoint saved>---0.8930540680885315\n","Epoch [14/50] - Batch [1/11] - Train Loss: 1.1241\n","Epoch [14/50] - Batch [2/11] - Train Loss: 3.3407\n","Epoch [14/50] - Batch [3/11] - Train Loss: 0.4629\n","Epoch [14/50] - Batch [4/11] - Train Loss: 1.1368\n","Epoch [14/50] - Batch [5/11] - Train Loss: 1.0899\n","Epoch [14/50] - Batch [6/11] - Train Loss: 0.9910\n","Epoch [14/50] - Batch [7/11] - Train Loss: 3.3700\n","Epoch [14/50] - Batch [8/11] - Train Loss: 0.6899\n","Epoch [14/50] - Batch [9/11] - Train Loss: 0.4970\n","Epoch [14/50] - Batch [10/11] - Train Loss: 0.3399\n","Epoch [14/50] - Batch [11/11] - Train Loss: 16.2299\n","Epoch [14/50] - Batch [1/3] - Test Loss: 2.0972\n","Epoch [14/50] - Batch [2/3] - Test Loss: 0.7159\n","Epoch [14/50] - Batch [3/3] - Test Loss: 1.1400\n","Epoch [14/50] - Train Loss: 2.6611, Test Loss: 1.3177\n","Epoch [15/50] - Batch [1/11] - Train Loss: 1.0269\n","Epoch [15/50] - Batch [2/11] - Train Loss: 0.9681\n","Epoch [15/50] - Batch [3/11] - Train Loss: 0.9510\n","Epoch [15/50] - Batch [4/11] - Train Loss: 0.5391\n","Epoch [15/50] - Batch [5/11] - Train Loss: 1.5018\n","Epoch [15/50] - Batch [6/11] - Train Loss: 0.3365\n","Epoch [15/50] - Batch [7/11] - Train Loss: 3.4959\n","Epoch [15/50] - Batch [8/11] - Train Loss: 2.3887\n","Epoch [15/50] - Batch [9/11] - Train Loss: 1.7518\n","Epoch [15/50] - Batch [10/11] - Train Loss: 1.8950\n","Epoch [15/50] - Batch [11/11] - Train Loss: 0.4623\n","Epoch [15/50] - Batch [1/3] - Test Loss: 1.9804\n","Epoch [15/50] - Batch [2/3] - Test Loss: 0.5490\n","Epoch [15/50] - Batch [3/3] - Test Loss: 1.6628\n","Epoch [15/50] - Train Loss: 1.3924, Test Loss: 1.3974\n","Epoch [16/50] - Batch [1/11] - Train Loss: 1.4889\n","Epoch [16/50] - Batch [2/11] - Train Loss: 1.3005\n","Epoch [16/50] - Batch [3/11] - Train Loss: 0.9059\n","Epoch [16/50] - Batch [4/11] - Train Loss: 1.4487\n","Epoch [16/50] - Batch [5/11] - Train Loss: 2.1860\n","Epoch [16/50] - Batch [6/11] - Train Loss: 1.4518\n","Epoch [16/50] - Batch [7/11] - Train Loss: 1.3492\n","Epoch [16/50] - Batch [8/11] - Train Loss: 1.5635\n","Epoch [16/50] - Batch [9/11] - Train Loss: 2.1242\n","Epoch [16/50] - Batch [10/11] - Train Loss: 0.3915\n","Epoch [16/50] - Batch [11/11] - Train Loss: 0.9421\n","Epoch [16/50] - Batch [1/3] - Test Loss: 2.9339\n","Epoch [16/50] - Batch [2/3] - Test Loss: 1.4145\n","Epoch [16/50] - Batch [3/3] - Test Loss: 3.9598\n","Epoch [16/50] - Train Loss: 1.3775, Test Loss: 2.7694\n","Epoch [17/50] - Batch [1/11] - Train Loss: 0.7752\n","Epoch [17/50] - Batch [2/11] - Train Loss: 0.5969\n","Epoch [17/50] - Batch [3/11] - Train Loss: 1.4863\n","Epoch [17/50] - Batch [4/11] - Train Loss: 2.0624\n","Epoch [17/50] - Batch [5/11] - Train Loss: 0.4889\n","Epoch [17/50] - Batch [6/11] - Train Loss: 2.9467\n","Epoch [17/50] - Batch [7/11] - Train Loss: 0.8572\n","Epoch [17/50] - Batch [8/11] - Train Loss: 1.7997\n","Epoch [17/50] - Batch [9/11] - Train Loss: 0.6615\n","Epoch [17/50] - Batch [10/11] - Train Loss: 0.9765\n","Epoch [17/50] - Batch [11/11] - Train Loss: 19.3551\n","Epoch [17/50] - Batch [1/3] - Test Loss: 1.1436\n","Epoch [17/50] - Batch [2/3] - Test Loss: 0.6945\n","Epoch [17/50] - Batch [3/3] - Test Loss: 1.0357\n","Epoch [17/50] - Train Loss: 2.9097, Test Loss: 0.9579\n","Epoch [18/50] - Batch [1/11] - Train Loss: 0.3924\n","Epoch [18/50] - Batch [2/11] - Train Loss: 1.4530\n","Epoch [18/50] - Batch [3/11] - Train Loss: 0.3103\n","Epoch [18/50] - Batch [4/11] - Train Loss: 0.7033\n","Epoch [18/50] - Batch [5/11] - Train Loss: 1.3960\n","Epoch [18/50] - Batch [6/11] - Train Loss: 1.0791\n","Epoch [18/50] - Batch [7/11] - Train Loss: 1.8467\n","Epoch [18/50] - Batch [8/11] - Train Loss: 2.0298\n","Epoch [18/50] - Batch [9/11] - Train Loss: 0.9668\n","Epoch [18/50] - Batch [10/11] - Train Loss: 1.1378\n","Epoch [18/50] - Batch [11/11] - Train Loss: 20.4332\n","Epoch [18/50] - Batch [1/3] - Test Loss: 0.6504\n","Epoch [18/50] - Batch [2/3] - Test Loss: 1.2253\n","Epoch [18/50] - Batch [3/3] - Test Loss: 0.7793\n","Epoch [18/50] - Train Loss: 2.8862, Test Loss: 0.8850\n","---<checkpoint saved>---0.8849938710530599\n","Epoch [19/50] - Batch [1/11] - Train Loss: 0.7058\n","Epoch [19/50] - Batch [2/11] - Train Loss: 0.3964\n","Epoch [19/50] - Batch [3/11] - Train Loss: 0.4223\n","Epoch [19/50] - Batch [4/11] - Train Loss: 1.0926\n","Epoch [19/50] - Batch [5/11] - Train Loss: 1.6131\n","Epoch [19/50] - Batch [6/11] - Train Loss: 1.4213\n","Epoch [19/50] - Batch [7/11] - Train Loss: 0.4961\n","Epoch [19/50] - Batch [8/11] - Train Loss: 0.8231\n","Epoch [19/50] - Batch [9/11] - Train Loss: 0.2914\n","Epoch [19/50] - Batch [10/11] - Train Loss: 0.6352\n","Epoch [19/50] - Batch [11/11] - Train Loss: 0.9581\n","Epoch [19/50] - Batch [1/3] - Test Loss: 1.0414\n","Epoch [19/50] - Batch [2/3] - Test Loss: 1.2559\n","Epoch [19/50] - Batch [3/3] - Test Loss: 0.7306\n","Epoch [19/50] - Train Loss: 0.8050, Test Loss: 1.0093\n","Epoch [20/50] - Batch [1/11] - Train Loss: 1.1354\n","Epoch [20/50] - Batch [2/11] - Train Loss: 0.4211\n","Epoch [20/50] - Batch [3/11] - Train Loss: 0.5596\n","Epoch [20/50] - Batch [4/11] - Train Loss: 0.6340\n","Epoch [20/50] - Batch [5/11] - Train Loss: 1.4355\n","Epoch [20/50] - Batch [6/11] - Train Loss: 0.6337\n","Epoch [20/50] - Batch [7/11] - Train Loss: 0.6874\n","Epoch [20/50] - Batch [8/11] - Train Loss: 0.8917\n","Epoch [20/50] - Batch [9/11] - Train Loss: 1.0162\n","Epoch [20/50] - Batch [10/11] - Train Loss: 1.3917\n","Epoch [20/50] - Batch [11/11] - Train Loss: 5.2378\n","Epoch [20/50] - Batch [1/3] - Test Loss: 1.5550\n","Epoch [20/50] - Batch [2/3] - Test Loss: 0.8651\n","Epoch [20/50] - Batch [3/3] - Test Loss: 0.2726\n","Epoch [20/50] - Train Loss: 1.2767, Test Loss: 0.8976\n","Epoch [21/50] - Batch [1/11] - Train Loss: 0.4650\n","Epoch [21/50] - Batch [2/11] - Train Loss: 0.4628\n","Epoch [21/50] - Batch [3/11] - Train Loss: 0.6583\n","Epoch [21/50] - Batch [4/11] - Train Loss: 0.2950\n","Epoch [21/50] - Batch [5/11] - Train Loss: 1.0654\n","Epoch [21/50] - Batch [6/11] - Train Loss: 1.2727\n","Epoch [21/50] - Batch [7/11] - Train Loss: 0.3716\n","Epoch [21/50] - Batch [8/11] - Train Loss: 0.8673\n","Epoch [21/50] - Batch [9/11] - Train Loss: 0.8460\n","Epoch [21/50] - Batch [10/11] - Train Loss: 1.3322\n","Epoch [21/50] - Batch [11/11] - Train Loss: 0.3368\n","Epoch [21/50] - Batch [1/3] - Test Loss: 0.6578\n","Epoch [21/50] - Batch [2/3] - Test Loss: 1.7856\n","Epoch [21/50] - Batch [3/3] - Test Loss: 0.6068\n","Epoch [21/50] - Train Loss: 0.7248, Test Loss: 1.0167\n","Epoch [22/50] - Batch [1/11] - Train Loss: 0.1698\n","Epoch [22/50] - Batch [2/11] - Train Loss: 1.3037\n","Epoch [22/50] - Batch [3/11] - Train Loss: 1.0572\n","Epoch [22/50] - Batch [4/11] - Train Loss: 0.6255\n","Epoch [22/50] - Batch [5/11] - Train Loss: 2.4175\n","Epoch [22/50] - Batch [6/11] - Train Loss: 0.2831\n","Epoch [22/50] - Batch [7/11] - Train Loss: 0.9488\n","Epoch [22/50] - Batch [8/11] - Train Loss: 0.3659\n","Epoch [22/50] - Batch [9/11] - Train Loss: 0.3769\n","Epoch [22/50] - Batch [10/11] - Train Loss: 0.9927\n","Epoch [22/50] - Batch [11/11] - Train Loss: 6.9756\n","Epoch [22/50] - Batch [1/3] - Test Loss: 0.7660\n","Epoch [22/50] - Batch [2/3] - Test Loss: 0.4417\n","Epoch [22/50] - Batch [3/3] - Test Loss: 1.2755\n","Epoch [22/50] - Train Loss: 1.4106, Test Loss: 0.8277\n","---<checkpoint saved>---0.8277420004208883\n","Epoch [23/50] - Batch [1/11] - Train Loss: 0.4848\n","Epoch [23/50] - Batch [2/11] - Train Loss: 0.6506\n","Epoch [23/50] - Batch [3/11] - Train Loss: 0.5926\n","Epoch [23/50] - Batch [4/11] - Train Loss: 0.8101\n","Epoch [23/50] - Batch [5/11] - Train Loss: 0.9874\n","Epoch [23/50] - Batch [6/11] - Train Loss: 0.1779\n","Epoch [23/50] - Batch [7/11] - Train Loss: 2.9334\n","Epoch [23/50] - Batch [8/11] - Train Loss: 0.4559\n","Epoch [23/50] - Batch [9/11] - Train Loss: 2.7467\n","Epoch [23/50] - Batch [10/11] - Train Loss: 0.6710\n","Epoch [23/50] - Batch [11/11] - Train Loss: 2.0393\n","Epoch [23/50] - Batch [1/3] - Test Loss: 1.3281\n","Epoch [23/50] - Batch [2/3] - Test Loss: 1.0111\n","Epoch [23/50] - Batch [3/3] - Test Loss: 0.5199\n","Epoch [23/50] - Train Loss: 1.1409, Test Loss: 0.9530\n","Epoch [24/50] - Batch [1/11] - Train Loss: 1.4462\n","Epoch [24/50] - Batch [2/11] - Train Loss: 0.9075\n","Epoch [24/50] - Batch [3/11] - Train Loss: 1.6418\n","Epoch [24/50] - Batch [4/11] - Train Loss: 0.7786\n","Epoch [24/50] - Batch [5/11] - Train Loss: 0.7777\n","Epoch [24/50] - Batch [6/11] - Train Loss: 1.4048\n","Epoch [24/50] - Batch [7/11] - Train Loss: 0.9083\n","Epoch [24/50] - Batch [8/11] - Train Loss: 1.5964\n","Epoch [24/50] - Batch [9/11] - Train Loss: 0.9722\n","Epoch [24/50] - Batch [10/11] - Train Loss: 0.7864\n","Epoch [24/50] - Batch [11/11] - Train Loss: 16.8570\n","Epoch [24/50] - Batch [1/3] - Test Loss: 0.6725\n","Epoch [24/50] - Batch [2/3] - Test Loss: 0.5351\n","Epoch [24/50] - Batch [3/3] - Test Loss: 0.6701\n","Epoch [24/50] - Train Loss: 2.5525, Test Loss: 0.6259\n","---<checkpoint saved>---0.6259081959724426\n","Epoch [25/50] - Batch [1/11] - Train Loss: 1.4451\n","Epoch [25/50] - Batch [2/11] - Train Loss: 0.8882\n","Epoch [25/50] - Batch [3/11] - Train Loss: 0.8444\n","Epoch [25/50] - Batch [4/11] - Train Loss: 0.4989\n","Epoch [25/50] - Batch [5/11] - Train Loss: 3.4056\n","Epoch [25/50] - Batch [6/11] - Train Loss: 1.2026\n","Epoch [25/50] - Batch [7/11] - Train Loss: 0.9552\n","Epoch [25/50] - Batch [8/11] - Train Loss: 2.0921\n","Epoch [25/50] - Batch [9/11] - Train Loss: 1.3351\n","Epoch [25/50] - Batch [10/11] - Train Loss: 0.6407\n","Epoch [25/50] - Batch [11/11] - Train Loss: 7.3564\n","Epoch [25/50] - Batch [1/3] - Test Loss: 1.7464\n","Epoch [25/50] - Batch [2/3] - Test Loss: 1.5401\n","Epoch [25/50] - Batch [3/3] - Test Loss: 1.0835\n","Epoch [25/50] - Train Loss: 1.8786, Test Loss: 1.4567\n","Epoch [26/50] - Batch [1/11] - Train Loss: 1.8219\n","Epoch [26/50] - Batch [2/11] - Train Loss: 1.7158\n","Epoch [26/50] - Batch [3/11] - Train Loss: 2.7016\n","Epoch [26/50] - Batch [4/11] - Train Loss: 0.8721\n","Epoch [26/50] - Batch [5/11] - Train Loss: 1.2271\n","Epoch [26/50] - Batch [6/11] - Train Loss: 2.0209\n","Epoch [26/50] - Batch [7/11] - Train Loss: 1.2547\n","Epoch [26/50] - Batch [8/11] - Train Loss: 0.5410\n","Epoch [26/50] - Batch [9/11] - Train Loss: 0.9302\n","Epoch [26/50] - Batch [10/11] - Train Loss: 0.8089\n","Epoch [26/50] - Batch [11/11] - Train Loss: 3.1777\n","Epoch [26/50] - Batch [1/3] - Test Loss: 0.8993\n","Epoch [26/50] - Batch [2/3] - Test Loss: 1.3395\n","Epoch [26/50] - Batch [3/3] - Test Loss: 1.3564\n","Epoch [26/50] - Train Loss: 1.5520, Test Loss: 1.1984\n","Epoch [27/50] - Batch [1/11] - Train Loss: 0.6687\n","Epoch [27/50] - Batch [2/11] - Train Loss: 0.5912\n","Epoch [27/50] - Batch [3/11] - Train Loss: 0.3879\n","Epoch [27/50] - Batch [4/11] - Train Loss: 2.0373\n","Epoch [27/50] - Batch [5/11] - Train Loss: 0.4156\n","Epoch [27/50] - Batch [6/11] - Train Loss: 0.4756\n","Epoch [27/50] - Batch [7/11] - Train Loss: 1.5133\n","Epoch [27/50] - Batch [8/11] - Train Loss: 0.8375\n","Epoch [27/50] - Batch [9/11] - Train Loss: 0.9227\n","Epoch [27/50] - Batch [10/11] - Train Loss: 0.4229\n","Epoch [27/50] - Batch [11/11] - Train Loss: 9.9176\n","Epoch [27/50] - Batch [1/3] - Test Loss: 1.4654\n","Epoch [27/50] - Batch [2/3] - Test Loss: 1.2336\n","Epoch [27/50] - Batch [3/3] - Test Loss: 1.1346\n","Epoch [27/50] - Train Loss: 1.6537, Test Loss: 1.2778\n","Epoch [28/50] - Batch [1/11] - Train Loss: 1.1685\n","Epoch [28/50] - Batch [2/11] - Train Loss: 1.0341\n","Epoch [28/50] - Batch [3/11] - Train Loss: 1.4451\n","Epoch [28/50] - Batch [4/11] - Train Loss: 0.6579\n","Epoch [28/50] - Batch [5/11] - Train Loss: 1.4896\n","Epoch [28/50] - Batch [6/11] - Train Loss: 1.9552\n","Epoch [28/50] - Batch [7/11] - Train Loss: 0.4450\n","Epoch [28/50] - Batch [8/11] - Train Loss: 0.6973\n","Epoch [28/50] - Batch [9/11] - Train Loss: 0.8946\n","Epoch [28/50] - Batch [10/11] - Train Loss: 1.1808\n","Epoch [28/50] - Batch [11/11] - Train Loss: 11.0889\n","Epoch [28/50] - Batch [1/3] - Test Loss: 2.1388\n","Epoch [28/50] - Batch [2/3] - Test Loss: 1.0415\n","Epoch [28/50] - Batch [3/3] - Test Loss: 3.0635\n","Epoch [28/50] - Train Loss: 2.0052, Test Loss: 2.0813\n","Epoch [29/50] - Batch [1/11] - Train Loss: 0.7807\n","Epoch [29/50] - Batch [2/11] - Train Loss: 0.5437\n","Epoch [29/50] - Batch [3/11] - Train Loss: 0.3677\n","Epoch [29/50] - Batch [4/11] - Train Loss: 0.8598\n","Epoch [29/50] - Batch [5/11] - Train Loss: 2.1716\n","Epoch [29/50] - Batch [6/11] - Train Loss: 0.5343\n","Epoch [29/50] - Batch [7/11] - Train Loss: 1.7518\n","Epoch [29/50] - Batch [8/11] - Train Loss: 0.6979\n","Epoch [29/50] - Batch [9/11] - Train Loss: 0.6125\n","Epoch [29/50] - Batch [10/11] - Train Loss: 0.1792\n","Epoch [29/50] - Batch [11/11] - Train Loss: 0.9639\n","Epoch [29/50] - Batch [1/3] - Test Loss: 1.3308\n","Epoch [29/50] - Batch [2/3] - Test Loss: 1.7231\n","Epoch [29/50] - Batch [3/3] - Test Loss: 1.7738\n","Epoch [29/50] - Train Loss: 0.8603, Test Loss: 1.6092\n","Epoch [30/50] - Batch [1/11] - Train Loss: 1.6118\n","Epoch [30/50] - Batch [2/11] - Train Loss: 0.3751\n","Epoch [30/50] - Batch [3/11] - Train Loss: 0.3387\n","Epoch [30/50] - Batch [4/11] - Train Loss: 1.5538\n","Epoch [30/50] - Batch [5/11] - Train Loss: 0.4501\n","Epoch [30/50] - Batch [6/11] - Train Loss: 1.2794\n","Epoch [30/50] - Batch [7/11] - Train Loss: 0.6342\n","Epoch [30/50] - Batch [8/11] - Train Loss: 0.7640\n","Epoch [30/50] - Batch [9/11] - Train Loss: 0.5888\n","Epoch [30/50] - Batch [10/11] - Train Loss: 0.4658\n","Epoch [30/50] - Batch [11/11] - Train Loss: 0.1601\n","Epoch [30/50] - Batch [1/3] - Test Loss: 0.4270\n","Epoch [30/50] - Batch [2/3] - Test Loss: 0.5061\n","Epoch [30/50] - Batch [3/3] - Test Loss: 1.4738\n","Epoch [30/50] - Train Loss: 0.7474, Test Loss: 0.8023\n","Epoch [31/50] - Batch [1/11] - Train Loss: 0.4659\n","Epoch [31/50] - Batch [2/11] - Train Loss: 0.4307\n","Epoch [31/50] - Batch [3/11] - Train Loss: 0.7958\n","Epoch [31/50] - Batch [4/11] - Train Loss: 0.7162\n","Epoch [31/50] - Batch [5/11] - Train Loss: 0.7021\n","Epoch [31/50] - Batch [6/11] - Train Loss: 0.8888\n","Epoch [31/50] - Batch [7/11] - Train Loss: 0.7087\n","Epoch [31/50] - Batch [8/11] - Train Loss: 0.3614\n","Epoch [31/50] - Batch [9/11] - Train Loss: 1.4977\n","Epoch [31/50] - Batch [10/11] - Train Loss: 0.7659\n","Epoch [31/50] - Batch [11/11] - Train Loss: 0.0122\n","Epoch [31/50] - Batch [1/3] - Test Loss: 0.4543\n","Epoch [31/50] - Batch [2/3] - Test Loss: 0.7849\n","Epoch [31/50] - Batch [3/3] - Test Loss: 0.3218\n","Epoch [31/50] - Train Loss: 0.6678, Test Loss: 0.5203\n","---<checkpoint saved>---0.5203076402346293\n","Epoch [32/50] - Batch [1/11] - Train Loss: 1.0670\n","Epoch [32/50] - Batch [2/11] - Train Loss: 0.5865\n","Epoch [32/50] - Batch [3/11] - Train Loss: 0.3822\n","Epoch [32/50] - Batch [4/11] - Train Loss: 1.0771\n","Epoch [32/50] - Batch [5/11] - Train Loss: 0.9145\n","Epoch [32/50] - Batch [6/11] - Train Loss: 1.2776\n","Epoch [32/50] - Batch [7/11] - Train Loss: 0.7988\n","Epoch [32/50] - Batch [8/11] - Train Loss: 0.3658\n","Epoch [32/50] - Batch [9/11] - Train Loss: 0.4363\n","Epoch [32/50] - Batch [10/11] - Train Loss: 0.3043\n","Epoch [32/50] - Batch [11/11] - Train Loss: 0.0014\n","Epoch [32/50] - Batch [1/3] - Test Loss: 0.2875\n","Epoch [32/50] - Batch [2/3] - Test Loss: 0.5352\n","Epoch [32/50] - Batch [3/3] - Test Loss: 4.2611\n","Epoch [32/50] - Train Loss: 0.6556, Test Loss: 1.6946\n","Epoch [33/50] - Batch [1/11] - Train Loss: 0.2693\n","Epoch [33/50] - Batch [2/11] - Train Loss: 0.5057\n","Epoch [33/50] - Batch [3/11] - Train Loss: 0.3921\n","Epoch [33/50] - Batch [4/11] - Train Loss: 2.7783\n","Epoch [33/50] - Batch [5/11] - Train Loss: 0.4762\n","Epoch [33/50] - Batch [6/11] - Train Loss: 0.9338\n","Epoch [33/50] - Batch [7/11] - Train Loss: 0.4619\n","Epoch [33/50] - Batch [8/11] - Train Loss: 0.2857\n","Epoch [33/50] - Batch [9/11] - Train Loss: 1.5107\n","Epoch [33/50] - Batch [10/11] - Train Loss: 0.4427\n","Epoch [33/50] - Batch [11/11] - Train Loss: 0.6525\n","Epoch [33/50] - Batch [1/3] - Test Loss: 3.4476\n","Epoch [33/50] - Batch [2/3] - Test Loss: 0.9374\n","Epoch [33/50] - Batch [3/3] - Test Loss: 0.4988\n","Epoch [33/50] - Train Loss: 0.7917, Test Loss: 1.6279\n","Epoch [34/50] - Batch [1/11] - Train Loss: 0.8115\n","Epoch [34/50] - Batch [2/11] - Train Loss: 1.1147\n","Epoch [34/50] - Batch [3/11] - Train Loss: 0.6848\n","Epoch [34/50] - Batch [4/11] - Train Loss: 1.7974\n","Epoch [34/50] - Batch [5/11] - Train Loss: 2.5307\n","Epoch [34/50] - Batch [6/11] - Train Loss: 0.9228\n","Epoch [34/50] - Batch [7/11] - Train Loss: 1.4974\n","Epoch [34/50] - Batch [8/11] - Train Loss: 0.6972\n","Epoch [34/50] - Batch [9/11] - Train Loss: 0.8290\n","Epoch [34/50] - Batch [10/11] - Train Loss: 0.4752\n","Epoch [34/50] - Batch [11/11] - Train Loss: 2.2827\n","Epoch [34/50] - Batch [1/3] - Test Loss: 1.2339\n","Epoch [34/50] - Batch [2/3] - Test Loss: 0.9246\n","Epoch [34/50] - Batch [3/3] - Test Loss: 0.5309\n","Epoch [34/50] - Train Loss: 1.2403, Test Loss: 0.8965\n","Epoch [35/50] - Batch [1/11] - Train Loss: 1.4983\n","Epoch [35/50] - Batch [2/11] - Train Loss: 0.7062\n","Epoch [35/50] - Batch [3/11] - Train Loss: 0.9425\n","Epoch [35/50] - Batch [4/11] - Train Loss: 0.5791\n","Epoch [35/50] - Batch [5/11] - Train Loss: 0.4963\n","Epoch [35/50] - Batch [6/11] - Train Loss: 0.4473\n","Epoch [35/50] - Batch [7/11] - Train Loss: 0.5500\n","Epoch [35/50] - Batch [8/11] - Train Loss: 0.5884\n","Epoch [35/50] - Batch [9/11] - Train Loss: 1.4793\n","Epoch [35/50] - Batch [10/11] - Train Loss: 0.5984\n","Epoch [35/50] - Batch [11/11] - Train Loss: 0.3629\n","Epoch [35/50] - Batch [1/3] - Test Loss: 0.7137\n","Epoch [35/50] - Batch [2/3] - Test Loss: 1.0846\n","Epoch [35/50] - Batch [3/3] - Test Loss: 0.6647\n","Epoch [35/50] - Train Loss: 0.7499, Test Loss: 0.8210\n","Epoch [36/50] - Batch [1/11] - Train Loss: 0.5400\n","Epoch [36/50] - Batch [2/11] - Train Loss: 0.7768\n","Epoch [36/50] - Batch [3/11] - Train Loss: 0.4240\n","Epoch [36/50] - Batch [4/11] - Train Loss: 0.5713\n","Epoch [36/50] - Batch [5/11] - Train Loss: 0.5019\n","Epoch [36/50] - Batch [6/11] - Train Loss: 0.5196\n","Epoch [36/50] - Batch [7/11] - Train Loss: 0.3209\n","Epoch [36/50] - Batch [8/11] - Train Loss: 0.9093\n","Epoch [36/50] - Batch [9/11] - Train Loss: 0.6232\n","Epoch [36/50] - Batch [10/11] - Train Loss: 0.3306\n","Epoch [36/50] - Batch [11/11] - Train Loss: 0.1299\n","Epoch [36/50] - Batch [1/3] - Test Loss: 1.4234\n","Epoch [36/50] - Batch [2/3] - Test Loss: 0.4227\n","Epoch [36/50] - Batch [3/3] - Test Loss: 2.8706\n","Epoch [36/50] - Train Loss: 0.5134, Test Loss: 1.5722\n","Epoch [37/50] - Batch [1/11] - Train Loss: 0.4140\n","Epoch [37/50] - Batch [2/11] - Train Loss: 1.4827\n","Epoch [37/50] - Batch [3/11] - Train Loss: 0.8748\n","Epoch [37/50] - Batch [4/11] - Train Loss: 0.1565\n","Epoch [37/50] - Batch [5/11] - Train Loss: 1.2804\n","Epoch [37/50] - Batch [6/11] - Train Loss: 0.8436\n","Epoch [37/50] - Batch [7/11] - Train Loss: 0.5002\n","Epoch [37/50] - Batch [8/11] - Train Loss: 0.5246\n","Epoch [37/50] - Batch [9/11] - Train Loss: 0.5528\n","Epoch [37/50] - Batch [10/11] - Train Loss: 0.4520\n","Epoch [37/50] - Batch [11/11] - Train Loss: 4.9014\n","Epoch [37/50] - Batch [1/3] - Test Loss: 1.1679\n","Epoch [37/50] - Batch [2/3] - Test Loss: 0.5812\n","Epoch [37/50] - Batch [3/3] - Test Loss: 0.9562\n","Epoch [37/50] - Train Loss: 1.0894, Test Loss: 0.9017\n","Epoch [38/50] - Batch [1/11] - Train Loss: 0.6848\n","Epoch [38/50] - Batch [2/11] - Train Loss: 1.0005\n","Epoch [38/50] - Batch [3/11] - Train Loss: 1.8592\n","Epoch [38/50] - Batch [4/11] - Train Loss: 0.4422\n","Epoch [38/50] - Batch [5/11] - Train Loss: 0.2601\n","Epoch [38/50] - Batch [6/11] - Train Loss: 0.8107\n","Epoch [38/50] - Batch [7/11] - Train Loss: 0.3481\n","Epoch [38/50] - Batch [8/11] - Train Loss: 0.3720\n","Epoch [38/50] - Batch [9/11] - Train Loss: 0.7176\n","Epoch [38/50] - Batch [10/11] - Train Loss: 0.3748\n","Epoch [38/50] - Batch [11/11] - Train Loss: 8.9311\n","Epoch [38/50] - Batch [1/3] - Test Loss: 0.4080\n","Epoch [38/50] - Batch [2/3] - Test Loss: 0.7674\n","Epoch [38/50] - Batch [3/3] - Test Loss: 1.5407\n","Epoch [38/50] - Train Loss: 1.4365, Test Loss: 0.9054\n","Epoch [39/50] - Batch [1/11] - Train Loss: 0.3280\n","Epoch [39/50] - Batch [2/11] - Train Loss: 0.4217\n","Epoch [39/50] - Batch [3/11] - Train Loss: 0.6626\n","Epoch [39/50] - Batch [4/11] - Train Loss: 1.3444\n","Epoch [39/50] - Batch [5/11] - Train Loss: 1.3824\n","Epoch [39/50] - Batch [6/11] - Train Loss: 4.8787\n","Epoch [39/50] - Batch [7/11] - Train Loss: 1.4412\n","Epoch [39/50] - Batch [8/11] - Train Loss: 0.7390\n","Epoch [39/50] - Batch [9/11] - Train Loss: 2.4128\n","Epoch [39/50] - Batch [10/11] - Train Loss: 1.4891\n","Epoch [39/50] - Batch [11/11] - Train Loss: 6.5439\n","Epoch [39/50] - Batch [1/3] - Test Loss: 5.7643\n","Epoch [39/50] - Batch [2/3] - Test Loss: 1.5375\n","Epoch [39/50] - Batch [3/3] - Test Loss: 2.0002\n","Epoch [39/50] - Train Loss: 1.9676, Test Loss: 3.1007\n","Epoch [40/50] - Batch [1/11] - Train Loss: 1.0156\n","Epoch [40/50] - Batch [2/11] - Train Loss: 1.5946\n","Epoch [40/50] - Batch [3/11] - Train Loss: 1.8189\n","Epoch [40/50] - Batch [4/11] - Train Loss: 1.9029\n","Epoch [40/50] - Batch [5/11] - Train Loss: 3.9078\n","Epoch [40/50] - Batch [6/11] - Train Loss: 4.2956\n","Epoch [40/50] - Batch [7/11] - Train Loss: 0.9925\n","Epoch [40/50] - Batch [8/11] - Train Loss: 2.4619\n","Epoch [40/50] - Batch [9/11] - Train Loss: 0.7719\n","Epoch [40/50] - Batch [10/11] - Train Loss: 1.3152\n","Epoch [40/50] - Batch [11/11] - Train Loss: 0.8154\n","Epoch [40/50] - Batch [1/3] - Test Loss: 2.9984\n","Epoch [40/50] - Batch [2/3] - Test Loss: 1.8908\n","Epoch [40/50] - Batch [3/3] - Test Loss: 1.3746\n","Epoch [40/50] - Train Loss: 1.8993, Test Loss: 2.0880\n","Epoch [41/50] - Batch [1/11] - Train Loss: 0.6633\n","Epoch [41/50] - Batch [2/11] - Train Loss: 1.6131\n","Epoch [41/50] - Batch [3/11] - Train Loss: 2.6242\n","Epoch [41/50] - Batch [4/11] - Train Loss: 0.8833\n","Epoch [41/50] - Batch [5/11] - Train Loss: 1.7485\n","Epoch [41/50] - Batch [6/11] - Train Loss: 0.2943\n","Epoch [41/50] - Batch [7/11] - Train Loss: 0.5753\n","Epoch [41/50] - Batch [8/11] - Train Loss: 1.7376\n","Epoch [41/50] - Batch [9/11] - Train Loss: 2.4720\n","Epoch [41/50] - Batch [10/11] - Train Loss: 0.6182\n","Epoch [41/50] - Batch [11/11] - Train Loss: 3.2700\n","Epoch [41/50] - Batch [1/3] - Test Loss: 0.6193\n","Epoch [41/50] - Batch [2/3] - Test Loss: 0.8461\n","Epoch [41/50] - Batch [3/3] - Test Loss: 0.3690\n","Epoch [41/50] - Train Loss: 1.5000, Test Loss: 0.6115\n","Epoch [42/50] - Batch [1/11] - Train Loss: 0.8222\n","Epoch [42/50] - Batch [2/11] - Train Loss: 0.8845\n","Epoch [42/50] - Batch [3/11] - Train Loss: 1.1908\n","Epoch [42/50] - Batch [4/11] - Train Loss: 1.5521\n","Epoch [42/50] - Batch [5/11] - Train Loss: 1.1152\n","Epoch [42/50] - Batch [6/11] - Train Loss: 0.6944\n","Epoch [42/50] - Batch [7/11] - Train Loss: 0.4742\n","Epoch [42/50] - Batch [8/11] - Train Loss: 1.4937\n","Epoch [42/50] - Batch [9/11] - Train Loss: 2.6186\n","Epoch [42/50] - Batch [10/11] - Train Loss: 0.3784\n","Epoch [42/50] - Batch [11/11] - Train Loss: 0.9292\n","Epoch [42/50] - Batch [1/3] - Test Loss: 2.7391\n","Epoch [42/50] - Batch [2/3] - Test Loss: 0.7525\n","Epoch [42/50] - Batch [3/3] - Test Loss: 0.9687\n","Epoch [42/50] - Train Loss: 1.1049, Test Loss: 1.4868\n","Epoch [43/50] - Batch [1/11] - Train Loss: 0.3913\n","Epoch [43/50] - Batch [2/11] - Train Loss: 1.4581\n","Epoch [43/50] - Batch [3/11] - Train Loss: 1.0073\n","Epoch [43/50] - Batch [4/11] - Train Loss: 1.6050\n","Epoch [43/50] - Batch [5/11] - Train Loss: 1.2733\n","Epoch [43/50] - Batch [6/11] - Train Loss: 0.4916\n","Epoch [43/50] - Batch [7/11] - Train Loss: 1.7804\n","Epoch [43/50] - Batch [8/11] - Train Loss: 1.2447\n","Epoch [43/50] - Batch [9/11] - Train Loss: 1.0091\n","Epoch [43/50] - Batch [10/11] - Train Loss: 0.6016\n","Epoch [43/50] - Batch [11/11] - Train Loss: 0.1755\n","Epoch [43/50] - Batch [1/3] - Test Loss: 0.7010\n","Epoch [43/50] - Batch [2/3] - Test Loss: 1.7781\n","Epoch [43/50] - Batch [3/3] - Test Loss: 1.1467\n","Epoch [43/50] - Train Loss: 1.0035, Test Loss: 1.2086\n","Epoch [44/50] - Batch [1/11] - Train Loss: 0.9180\n","Epoch [44/50] - Batch [2/11] - Train Loss: 0.4498\n","Epoch [44/50] - Batch [3/11] - Train Loss: 1.1036\n","Epoch [44/50] - Batch [4/11] - Train Loss: 1.1383\n","Epoch [44/50] - Batch [5/11] - Train Loss: 1.1580\n","Epoch [44/50] - Batch [6/11] - Train Loss: 0.3450\n","Epoch [44/50] - Batch [7/11] - Train Loss: 2.7372\n","Epoch [44/50] - Batch [8/11] - Train Loss: 1.9059\n","Epoch [44/50] - Batch [9/11] - Train Loss: 0.5347\n","Epoch [44/50] - Batch [10/11] - Train Loss: 0.7785\n","Epoch [44/50] - Batch [11/11] - Train Loss: 12.0207\n","Epoch [44/50] - Batch [1/3] - Test Loss: 0.5851\n","Epoch [44/50] - Batch [2/3] - Test Loss: 0.6768\n","Epoch [44/50] - Batch [3/3] - Test Loss: 0.6435\n","Epoch [44/50] - Train Loss: 2.0991, Test Loss: 0.6351\n","Epoch [45/50] - Batch [1/11] - Train Loss: 1.3668\n","Epoch [45/50] - Batch [2/11] - Train Loss: 0.6801\n","Epoch [45/50] - Batch [3/11] - Train Loss: 3.8310\n","Epoch [45/50] - Batch [4/11] - Train Loss: 0.7725\n","Epoch [45/50] - Batch [5/11] - Train Loss: 1.7202\n","Epoch [45/50] - Batch [6/11] - Train Loss: 1.1850\n","Epoch [45/50] - Batch [7/11] - Train Loss: 1.1398\n","Epoch [45/50] - Batch [8/11] - Train Loss: 1.0693\n","Epoch [45/50] - Batch [9/11] - Train Loss: 0.8789\n","Epoch [45/50] - Batch [10/11] - Train Loss: 1.0943\n","Epoch [45/50] - Batch [11/11] - Train Loss: 4.1233\n","Epoch [45/50] - Batch [1/3] - Test Loss: 0.8441\n","Epoch [45/50] - Batch [2/3] - Test Loss: 1.7483\n","Epoch [45/50] - Batch [3/3] - Test Loss: 0.4882\n","Epoch [45/50] - Train Loss: 1.6237, Test Loss: 1.0269\n","Epoch [46/50] - Batch [1/11] - Train Loss: 0.6819\n","Epoch [46/50] - Batch [2/11] - Train Loss: 1.8799\n","Epoch [46/50] - Batch [3/11] - Train Loss: 1.4732\n","Epoch [46/50] - Batch [4/11] - Train Loss: 0.9756\n","Epoch [46/50] - Batch [5/11] - Train Loss: 2.3408\n","Epoch [46/50] - Batch [6/11] - Train Loss: 0.4884\n","Epoch [46/50] - Batch [7/11] - Train Loss: 0.7053\n","Epoch [46/50] - Batch [8/11] - Train Loss: 1.1652\n","Epoch [46/50] - Batch [9/11] - Train Loss: 1.6604\n","Epoch [46/50] - Batch [10/11] - Train Loss: 0.8450\n","Epoch [46/50] - Batch [11/11] - Train Loss: 5.3835\n","Epoch [46/50] - Batch [1/3] - Test Loss: 1.7086\n","Epoch [46/50] - Batch [2/3] - Test Loss: 0.6333\n","Epoch [46/50] - Batch [3/3] - Test Loss: 0.7097\n","Epoch [46/50] - Train Loss: 1.5999, Test Loss: 1.0172\n","Epoch [47/50] - Batch [1/11] - Train Loss: 1.3732\n","Epoch [47/50] - Batch [2/11] - Train Loss: 0.5423\n","Epoch [47/50] - Batch [3/11] - Train Loss: 0.6365\n","Epoch [47/50] - Batch [4/11] - Train Loss: 2.8584\n","Epoch [47/50] - Batch [5/11] - Train Loss: 2.3804\n","Epoch [47/50] - Batch [6/11] - Train Loss: 0.6868\n","Epoch [47/50] - Batch [7/11] - Train Loss: 0.7978\n","Epoch [47/50] - Batch [8/11] - Train Loss: 0.3875\n","Epoch [47/50] - Batch [9/11] - Train Loss: 0.4496\n","Epoch [47/50] - Batch [10/11] - Train Loss: 0.8208\n","Epoch [47/50] - Batch [11/11] - Train Loss: 6.7351\n","Epoch [47/50] - Batch [1/3] - Test Loss: 1.0104\n","Epoch [47/50] - Batch [2/3] - Test Loss: 0.6513\n","Epoch [47/50] - Batch [3/3] - Test Loss: 0.5899\n","Epoch [47/50] - Train Loss: 1.6062, Test Loss: 0.7505\n","Epoch [48/50] - Batch [1/11] - Train Loss: 0.6687\n","Epoch [48/50] - Batch [2/11] - Train Loss: 0.7746\n","Epoch [48/50] - Batch [3/11] - Train Loss: 2.5953\n","Epoch [48/50] - Batch [4/11] - Train Loss: 0.4694\n","Epoch [48/50] - Batch [5/11] - Train Loss: 1.5198\n","Epoch [48/50] - Batch [6/11] - Train Loss: 0.6709\n","Epoch [48/50] - Batch [7/11] - Train Loss: 0.4970\n","Epoch [48/50] - Batch [8/11] - Train Loss: 0.6913\n","Epoch [48/50] - Batch [9/11] - Train Loss: 0.3960\n","Epoch [48/50] - Batch [10/11] - Train Loss: 1.0785\n","Epoch [48/50] - Batch [11/11] - Train Loss: 0.0004\n","Epoch [48/50] - Batch [1/3] - Test Loss: 2.7257\n","Epoch [48/50] - Batch [2/3] - Test Loss: 2.0266\n","Epoch [48/50] - Batch [3/3] - Test Loss: 2.9342\n","Epoch [48/50] - Train Loss: 0.8511, Test Loss: 2.5622\n","Epoch [49/50] - Batch [1/11] - Train Loss: 1.3244\n","Epoch [49/50] - Batch [2/11] - Train Loss: 0.6761\n","Epoch [49/50] - Batch [3/11] - Train Loss: 0.5678\n","Epoch [49/50] - Batch [4/11] - Train Loss: 2.1374\n","Epoch [49/50] - Batch [5/11] - Train Loss: 0.8324\n","Epoch [49/50] - Batch [6/11] - Train Loss: 0.3757\n","Epoch [49/50] - Batch [7/11] - Train Loss: 0.6528\n","Epoch [49/50] - Batch [8/11] - Train Loss: 0.5994\n","Epoch [49/50] - Batch [9/11] - Train Loss: 0.5421\n","Epoch [49/50] - Batch [10/11] - Train Loss: 0.3410\n","Epoch [49/50] - Batch [11/11] - Train Loss: 0.2272\n","Epoch [49/50] - Batch [1/3] - Test Loss: 0.5499\n","Epoch [49/50] - Batch [2/3] - Test Loss: 0.7703\n","Epoch [49/50] - Batch [3/3] - Test Loss: 1.2186\n","Epoch [49/50] - Train Loss: 0.7524, Test Loss: 0.8462\n","Epoch [50/50] - Batch [1/11] - Train Loss: 0.4912\n","Epoch [50/50] - Batch [2/11] - Train Loss: 0.7731\n","Epoch [50/50] - Batch [3/11] - Train Loss: 0.4092\n","Epoch [50/50] - Batch [4/11] - Train Loss: 0.7534\n","Epoch [50/50] - Batch [5/11] - Train Loss: 0.4448\n","Epoch [50/50] - Batch [6/11] - Train Loss: 1.8053\n","Epoch [50/50] - Batch [7/11] - Train Loss: 0.3845\n","Epoch [50/50] - Batch [8/11] - Train Loss: 0.4206\n","Epoch [50/50] - Batch [9/11] - Train Loss: 0.3911\n","Epoch [50/50] - Batch [10/11] - Train Loss: 2.1005\n","Epoch [50/50] - Batch [11/11] - Train Loss: 1.1527\n","Epoch [50/50] - Batch [1/3] - Test Loss: 0.5400\n","Epoch [50/50] - Batch [2/3] - Test Loss: 0.4139\n","Epoch [50/50] - Batch [3/3] - Test Loss: 0.4331\n","Epoch [50/50] - Train Loss: 0.8297, Test Loss: 0.4624\n","---<checkpoint saved>---0.46236006418863934\n"]}],"source":["NUM_EPOCHS = 50\n","CHECKPOINT_PATH = 'checkpoint.pth'\n","best_loss = 1e9\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    train_loss = 0.0\n","\n","    # Initialize counters for debugging\n","    total_train_batches = len(train_loader)\n","    train_batch_count = 0\n","\n","    for images, labels in iter(train_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = F.mse_loss(outputs, labels)\n","        train_loss += float(loss)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print detailed training information for each batch\n","        train_batch_count += 1\n","        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Batch [{train_batch_count}/{total_train_batches}] - Train Loss: {loss:.4f}\")\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    test_loss = 0.0\n","\n","    # Initialize counters for debugging\n","    total_test_batches = len(test_loader)\n","    test_batch_count = 0\n","\n","    for images, labels in iter(test_loader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = model(images)\n","        loss = F.mse_loss(outputs, labels)\n","        test_loss += float(loss)\n","\n","        # Print detailed testing information for each batch\n","        test_batch_count += 1\n","        print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Batch [{test_batch_count}/{total_test_batches}] - Test Loss: {loss:.4f}\")\n","\n","    test_loss /= len(test_loader)\n","\n","    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}] - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n","\n","    if test_loss < best_loss:\n","        torch.save(model.state_dict(), CHECKPOINT_PATH)\n","        best_loss = test_loss\n","        print(f\"---<checkpoint saved>---{best_loss}\")\n"]},{"cell_type":"markdown","metadata":{"id":"q_h0o4slNjFe"},"source":["Once the model is trained, it will generate ``checkpoint.pth`` file will save the model with lowest loss function. And this model can be used for prediction."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}